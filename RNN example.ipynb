{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f073c01f198>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEVCAYAAADkckIIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzsnXd4FOXWwH+HXhSpKkUJCiIdLwHFLpYPG6BiQWkKothQAakC0gRR0GtBsVzFBui9Yi9UKygBRJp0UEAgdOmQnO+PM8ElpGyyZXaT9/c8+yQ7887MmdndOXPe00RVcTgcDocjjQJ+C+BwOByO2MIpBofD4XAcg1MMDofD4TgGpxgcDofDcQxOMTgcDofjGJxicDgcDscxOMXgCBoRmSgiP+Ziu44ioiJyaS6PW1xENorIwNxsHy1E5FLvPDv6LUssIiJNRCRVRJrGgCwJInJIRG7yW5ZYxCmGKBFw0+jhtywZISIPZ3VDE5ELgFuA/lETykNV9wMjgJ4iUjHaxw8nAd+DYF5r/ZY3WLyb/iARqZTFsDHAl6o6K1pyZYaqrgXeBEaKSBF/pYk9CvktgCNmeBhYi/1YMmIA8KuqzsjFvt8GJgCHciWZ8TowDHgU6BnCfvxmKdAu3bIuwEXAI8DWgOV7oiVUGGgCDAQ+AzamXykiVwLnA1dEWa6seBa4G2gDvOWzLDGFUwyObBGR6sCVQPfcbK+qKUBKKDKo6l4R+R/QUUT6q+rBUPbnF6q6GXgncJmIXIEphsnek2xYEZFCQGHP8vKL+4D1QG4eLCKCqi4RkXnAvTjFcAxuKslHvHlO9Uzw60RkjogcEJG/RGSU94MOHD9TRNaKyBki8rGI7BKR3SLykYickW5spvP6afsJeK9AVeCSdFMZCd6Q1oAAX2Swr/NF5EsR2eTJvkFEvhCR87KSJWBZMxHpISKrROSgiCwXkQ6ZXLIvgfLAZVlc1rT9FxCRfiLynSfbIRH5Q0TGiki5dGNz9Dl427QUkfneuD9FZAhQODu5coOIVBWR50TkNxHZKSL7RWShN/1XIN3YB7xzuUBEhnqf8wHg2oAxj4jISk/2pSLSOWC7xHT7Kycio0VktXcNN4vIeBE5LWDM08Dz3ts5Ad+fF7z1Jbzjf6WqqRmcX3ER6e+d034R2SEiP4tI53TjaojIBBHZEvBdGSQiRdONO1lEXhSRNd45bvU+0wczuLxfAueJSJVsP4h8hLMYYoNrsCeql4E3gJZAD2AHMDzd2JLATOBnoA9Qw9v2PBE5R1U35eL47bD5363YdE0ayd7fS4CdwPLAjUSkJjAF2AQ8B2wGTgEuBBoAs4M49nCgOPAKcBDoCrwpIitVNb2jO21u+lLgq2z2WwSbcvov8DGwF2gMdAIuFJFGqpp+aiuoz0FEbvD2uxYYDBwB7iTg5htmGnuyfQysAooB12Of2WlkbMm96P19CTv3NZ7sQzA/0WzgBaCUdw4ZTf+U98ZVwKbyfveO1xW4wruGfwHveWPaY1OOa7xdLPP+NsWU5i8ZHKM4ZkWcC3yOXfcU7PvTCnjNG3cW9p0v6p3bWsyKHQicKyLX6D+F3z71th8LLAZOAOpg3+M0BZZG4HfqHRyGqrpXFF7YF0+BHgHLErxle4GEgOUCLAL+SrePmd74Z9Mtv8Fb/nLAso7eskszkGUmsDbdsrXAzExkXwfMy2D5Q94xmmRz7sfJErBsPlAkYHllTEG8n8m+DgOfBnG9BSiewfJO3nFvyc3nABQE/sCUaPmA5Sd510mBjjn8brzpbZeQyfoSgGSw/CPvWpUJWPaAt69fgaLpxlfyrt/P2NRS2vKqwH5vu8SA5a8DfwM10+3nLG/8CxkcNzEDOR/01jXLYN1gb13fDNYVCPj/Y2/cJenGjPWW3xpwjgo8FeS1P9sb/2ROPrO8/nJTSbHBMXPLat/YGcCpInJCBuNHBL5R1Y+wp7NWEZKvArA9g+W7vL8tRaRYLvf9kgY8uavqBswyqZHJ+O3AydntVI39ACJSUERKe0/A070h52awWTCfQyPsqfk/qro1YOwuzNIIO6q6z5MFESkqImW9c/kKs4waZrDZC3q8H+ZqbJbgBVU9HLD/dcAHgQO96bNbMYtwm4iUT3thn8E84KogT6GC9zej79AdwF/AU+lXqDft5H23rga+V9Vv0w0b6v29wfu7B7M4Lghyemib9zfb71R+wimG2GB1BsvSvrDl0i3fqRlPFy0FThGRkmGVzFDs6Tk9E4CpQF9gu4hMF5FeIlI1B/vO7NzTn3ca4smTLSJyi4j8jD3d7sCmxtKOVyYHshAgT5ov5/cMxi4JRq6c4imDwSKyCvMXbMPOJU0RZXQuyzNYVs37uyyDdemXnY5NW97gHSv963xs2jAY0j6vY75DIlLQk2mRqh7JYvvK2FTU4uN2bA8Su/A+F1XdDfTCoqT+8Pwyz4rIRZnsO00m138gAOdjiA2yitjJ6IYcDFl90XP6uScDZY87gD2RXikiTYD/Ay7GpgYGicjtniWTHZmde2bnXYZ/fB+ZIiI3AhOxee1uwJ/YTbUg9qSd0UNRJD6HcDAW82G8DQzCzv8IcIH3PqNz2RfiMdPO91Pg35mMyepmHkja53XcdygSqOozIjIJ8/lcDNwOdBOR11W1c7rhaTJl+53KTzjFEH+UFpFTM7AaagFbVHWv9z7NbM/ox1gNm2sOJCtFsgi4WEQKaAZRJar6C55j0YtWmY+Z+MEohqARi5Iq5MmTHe0wRXCZqh69SYrI2SGKkWZVZLSf2iHu+zi8qKPbgS9UtX26dRlNIWXFWu9vTY53BNdM9349ZmmVVNWpQew7u+8P2PTgtKMbqKaIyGqgrogUysJq2IDlwNRJv0Isoe4k0ll7qvonZlG9LCKFsamyTiLyjKouDRhaPZ2MDtxUUrzSO/CNFyVTE5gcsDhtKuGKdGPbYA669Owh8ye6mcCJpLvxefPN6VlPJhZGGEgLgU0/z5wRKdjN6uh3XESE0DO352LneGfg+YtIKSwePhKkcPw0TGnMqZsTvvT29YB3s0zbV1Xg5sCBnjU4CWgmIs0z2pmIBM7LpyXjZfS5z8ac5OdlsO5doCIZRFZ5nxeqegCz8i7KYEqor/f3I2+bE9L7uzx/StqNP718OflO5RucxRB/bAVu9J6UZvJPuOpmbFoBAFVdJiJTgXu8H9ivmJPyBmAlx8fcz8aeqIZg/opULPpnLxaaORILmQx8suovIldh2a5rsJvX9djT9HHOxDBwDXb+wSRJfQjcBEwXkfHY+bbCInxyjfeU+wh20/xFRF7FplTuwub+Tw9l/xkcL1VEPgLu8M7je+xG2hn7zIM+nqpuEJGR2M30OxGZgIWrdsU+10SOffLvjhdGKiLvY1ZGChbFdR329P+AN/Zn7+8gz+m7D1iuqvNU9YCIfAr8n4gUVEt4TGME5lgeIVZ2ZTp2PethDzDXe+N6YtNCX4lIWrjqFdj3+Sv+cZ43BD4VS4ZcgoVZ18OU9u8cbyldA8xW1fVBXcT8gt9hUfnlRdbhqoMyGD+IdCGMeGGmmKPtY2A3Fk74MVA9g32civ1gdmNPdF9iU04zOT5c9WRMAWzHlEL6Y38BLMzgnCZ6Mu33tv0Zu2lJwLiOZB6uemkGcmckX0nvHEbl4Jrfjd0cDmCRL+OwJ0YF3szt5+AtvxFTtgcx/8UQLK4+EuGqJ2Lz/Gl+kt+x8hnXe9u1Dhibadiot16wG/4qT/bfvc+rt7dd7XTjS3nnlnYdd3v/jwX+lW7svZgT+5C3r8Bw1mbesv/LQKYSwBOeLAexQIHZwF3pxp2FBTxs9Y6xwtuuaMCYU7FchYWYU3ofZj2PAiqk218dT6YOft8fYu0l3gVyxAEiMhO7eST4cOymwE/AlRrcnHO4j98NS76roZZU5QgjIvIfoANQWi2yJxLH+B7Yq6oZTk1FG8/auwxThqHU8cpzOB+DIyjUKmJOxKKOooqXHdsbsxacUggB71qmX5aA5Sz8HCml4PEocJWInB/BYwSFiFTDFGEvpxSOx1kMcYSfFoMjbyAirTEfw2Rseu1MrLrricBVmrvquY48hnM+Oxz5i6VYXaSuWNLePswvNFRVv/dTMEfs4CwGh8PhcByD8zE4HA6H4xicYnA4HA7HMTjF4HA4HI5jcIrB4XA4HMfgFIPD4XA4jsEpBofD4XAcg1MMDofD4TgGpxgcDofDcQxOMTgcDofjGOKyJEb58uU1ISHBbzEcDocjrpg7d+5WVa2Q3bi4VAwJCQkkJSX5LYbD4XDEFSKyLphxbirJ4XA4HMfgFIPD4XA4jsEpBofD4XAcg1MMDofD4TgGpxgcDofDcQxhUQwi8oaIbBGRRZmsFxH5t4isFJHfRORfAes6iMgK79UhHPI4HA6HI/eEy2J4E2iexfqrgRreqwswFkBEygIDgXOBJsBAESkTJpkcDofDkQvCksegqt+JSEIWQ1oC49X6iM4WkdIiUhG4FJiiqtsBRGQKpmDeD4dcx/H227BuHZQpY6/KlaFBAyhdOiKHizf27YNly2DpUtixAw4csFfx4lCxor1q1rS/DocjwqSk2A9y2TLYvt1+lDt2QPfuULZsRA8drQS3ysCfAe/Xe8syW34cItIFszY4/fTTcyfFxInw+efHLz/jDDjvPGjVCq6+Gk44IXf7jzMOHIBvv4UvvoCvvoIVKyCYFuAJCXD++XD55XDjjU6vOhxhQRUWLID//Q+mTrX/9+07dkyBAnDHHXlGMYSMqo4DxgEkJiYGcfvKgM8+g4MHYedOe61ZA/Pnw7x5MGUKvPceFCsG114LDz0EF10EIuE8jZhgyRJ46SUYPx7+/ttO+bLL7PtWq5a9Tj7ZLIWiRWHvXvjrL9i4ERYuhJ9+ghkz7HJ17WqXq2NHuO46+946HI4csHUrjB0Lb74Jq1fbj+i886BLFzjnHKhbF8qXtyewE0+Mzj1JVcPyAhKARZmsewVoE/B+GVARaAO8ktm4zF6NGjXSsHPkiOrMmaoPPqharpwqqCYmqk6cqJqSEv7j+cAPP6g2a2anVqSIart2ql98obpvX873lZqqOmeO6sMPq556qu2zTh3Vd95RPXw4/LI7HHmOVatU771XtXhx+wFdeaXqq6+qbt4csUMCSRrM/TyYQUHtKGvFcC3wJSDAecAv3vKywBqgjPdaA5TN7lgRUQyB7N2rOnasao0adomaNFGdNSuyx4wgS5eqtmplp3LqqapPPqm6ZUv49n/4sOq775piANWzzlL96qvw7d/hyFPs2qX62GP2dFakiGqnTqqLF0fl0FFVDJiz+C/gMOYn6ATcC9zrrRfgRWAVsBBIDNj2LmCl97ozmONFXDGkkZKi+uabqhUr2qW64w7V5OToHDsMHDig2q+fasGCqieeqDp0qOqePZE7XkqK6kcf/aNPW7dW/fPPyB3P4YgrUlNV335b9ZRT7AfSvr3q+vVRFSHqFkM0X1FTDGn8/bdq376qhQvbI/cXX0T3+LkgKUm1bl37hDt0CK+FkB0HDpgSKlZM9YQT7LfgcORrtm5Vvekm+0Gee67qzz/7IkawisG5CoPhhBNg2DCYM8ecQNdcA/ffb2E9MYYqPP00nHuuRbh99pn5tCpkW4E9fBQtCv36mZO7YUNo1w46dDBHt8OR75gyBerVg08+gREj4McfoUkTv6XKEqcYckKDBqYcune3sJ6LL4YNG/yW6ih79sCtt0LPnhZ5u2iRRQz5RbVqFr00cCC88w40agS//+6fPA5HVEl7Smve3PKmfv4ZevWCggX9lixbnGLIKcWK2Yf90UeWCda4sX3gPrN6tVkJ//0vjBwJH3xg30W/KVQIBg0yBbFrFzRtCtOm+S2VwxFh9u+H9u3tKe2mm+CXXyz0NE5wiiG3tGoFs2aZorj4YvjwQ99EmT/fEs42bYKvv4bHHou99IuLLzb9WaWKPUC9+qrfEjkcEWL7dmjWzMzkIUMssbZkSb+lyhFOMYRC3bo2tZSYaHM4b7wRdRFmzIBLLoEiReCHH+CKK6IuQtAkJNj06hVXWO7OsGF+S+RwhJmNG+0paP58M9/794+9p7QgcIohVMqVg2++gSuvhE6dYPToqB3644/t6fv00y0buVatqB0615QqBZ9+ag7p/v3tpbnLY3c4Yos1a6xawrp18OWXVi8mTombkhgxTcmSFnFwxx3mmD54EPr0ieghP/sMbr4Z/vUvq3UU4dIpYaVQIYuUKlbMrIb9+81tE4cPVg6HsXq1WQr795sTLcajjrLDKYZwUaQITJhgDqe+fU1ZPPRQRA711Vfmz2rQwHwKJ50UkcNElAIF4JVXTDmMHg2FC1skn8MRd6xfbxUl9++HmTMtNDXOcYohnBQsCG+9ZV+Qbt1MOXTqFNZDTJtmfu86dWwGKx6VQhoi8NxzcOSIRVKVKWPRfA5H3LB5symF7dth+vQ8oRTAKYbwU6gQvP++3b3vvtuqId5yS1h2/euvcMMNUKOG5czEQjhqqIjACy9YsdvevW1K7O67/ZbK4QiCXbvgqqvMYvjmG0vUySM4xRAJiha1iISrrrKppcqV4YILQtrlH39YwvVJJ5lfq1y5MMkaAxQoYIbWrl1wzz2Wpd2qld9SORxZcPgwtG5t6f1ffBHy7zvWcFFJkaJECZg82UKGWraElStzvasdOyz6aN8+UwpVqoRRzhihcGFLymvSxHz48+b5LZHDkQmq1ohk6lRLyLnySr8lCjtOMUSS8uXtaQLscX/bthzv4sgRezBZtcr0TN26YZYxhkjTpeXLw/XXx1S1EYfjH558El5/3WKtO3b0W5qI4BRDpKle3RIO/vjDfA1HjuRo8x49zKc1bhxcemlkRIwlTj3V8hx27zblsHev3xI5HAFMnmwVIm+/HQYP9luaiOEUQzS44AKLzZw+PUf5DW+9ZVE73bpZddL8Qv36Fvm7YAF07uwS4Bwxwu+/m8+wcWOzGPJw4o1TDNGiQwcr1f3001Y7JRt++cUcsc2a2Sb5jWuvtTIzEyZY1JLD4Su7d1tIYLFiFlhSrJjfEkWUsCgGEWkuIstEZKWI9M5g/RgR+dV7LReRnQHrUgLWfRIOeWKW0aPNerjrLli4MNNh27aZX6FiRdMhhfJp7Fjv3jad9OijVvLD4fAFVfMlrFgBkybBaaf5LVHECVkxiEhBrG3n1UBtoI2I1A4co6qPqGpDVW0IPA/8L2D1/rR1qtoiVHlimiJFLPTmpJOsnsWePccNSfsObtpkQ8uXj76YsUKBAjB+PFStapdr82a/JXLkS5591srsjxqVPxx9hMdiaAKsVNXVqnoImAC0zGJ8G6xHdP6kYkV47z1YvhwefPC41aNHWx2kZ56xoq35ndKlzXLfvt2md1NT/ZbIka9ISrJ0/Fat4OGH/ZYmaoRDMVQG/gx4v95bdhwiUhWoBkwPWFxMRJJEZLaIZJrWJCJdvHFJycnJYRDbRy69FB5/3CrJvfPO0cWzZ9v0yY03wgMP+CZdzNGggT20ffMNjBnjtzSOfMPu3XDbbRYql8edzemJtvP5NuBDVU0JWFZVVROB24FnReTMjDZU1XGqmqiqiRWi2cA4Ujz+uJXo7doVVqxg925o08aS1/LZdzAounQxhdmnD8yd67c0jjyPKtx7L6xdayVu4ql8cRgIh2LYAAR6Y6p4yzLiNtJNI6nqBu/vamAmED/970KhUCGbUipSBG6/nW4PpvLHH7aodGm/hYs9RCzJ9JRTTIFm4J5xOMLHO++YQhg0KM+VuwiGcCiGOUANEakmIkWwm/9x0UUicjZQBpgVsKyMiBT1/i8PXAAsCYNM8UGVKjBuHP9LOo03xxegb1/riezImLJl7fe6ciU88ojf0jjyLH/+aXO5F14Y8b4qsUrIikFVjwAPAF8DS4FJqrpYRAaLSGCU0W3ABNVj0pVqAUkisgCYAYxQ1fyjGIC/zr+JLkXfIpE5DLjWzZFkxyWXWE/r116zulEOR1hJTYU774SUFMswLVjQb4l8QTQO00oTExM1KSnJbzFCRtUSuWbOVOafeAk1yyZb9bjixf0WLaY5eNAqHO/YAYsW5Y3y444Y4fnnrcHWuHF5sv67iMz1fLpZ4jKffWT8eHvqHTlSqPnO45Zy37ev32LFPEWL2sPc5s0Ra5LnyI8sX27m6DXXWC2WfIxTDD6xcaOFRV90kVXK4Mor7Z/nnoNZs7LdPr/TqJEVt3znHatr5nCERGqqdVssXtzmKfN5WKBTDD6QVs79wAELTS2Q9ik8+aSl23fqZPMljizp1w8aNrRruXNn9uMdjkx5+WX44QdLlKlY0W9pfMcpBh+YMAE++QSGDrU2nUc58USrwrp0qa10ZEnhwqZYk5NtBsDhyBXr1ll2c1rHRYdTDNFm61abFz/33Ewy7Js3ty/niBFWd9qRJf/6lxXZe/VVmDnTb2kccUdaIpuqPZTl8ymkNJxiiDKPPWbTHq++mkUk3OjRFrTfqZOFzTmyZNAgOOMMy47ev99vaRxxxbvvwldf2TRuQoLf0sQMTjFEkW+/hf/8x7qy1auXxcBy5cwJPXcujB0bNfnilRIlLLpwxQrr4eBwBMWOHWZuNmkC993ntzQxhctjiBIHD5qj9OBBi70vUSKbDVRtzvOXXyyM1TnEsqVjRyspsmAB1KrltzSOmKdrV3uiSEqCc/JHJR6XxxBjPPWU3d9feikIpQA21/nSS6ZJHn004vLlBZ56CkqWtKjfOHzecUST2bPNp/DQQ/lGKeQEpxiiwKpVMGwY3HKL+ZaDpkYNq9UyYQJMmRIx+fIKJ59sU8UzZlj9M4cjQ44cMYdzpUoweLDf0sQkbiopCrRoYTerZcvsu5gjDhz4xyGxaJGl/ToyJSXFChH+8Ydd75NO8lsiR8zx3HMWEvjBB9ZDNx/hppJihM8/h08/hQEDcqEUwJqOv/CClRR1XWqypWBB89dv2WItLxyOY9iyBQYONP/dTTf5LU3M4iyGCHLgANSta4lYCxZY64Vc06oVTJ1qj8GVM2yQ5wjgvvvMr/jrr/YZOByAhYCPHw8LF8LZZ/stTdRxFkMMMHq0+Rf+/e8QlULazo4cgZ49wyJbXmfIEChVCrp1c45oh8cvv8Abb9g0Uj5UCjnBKYYIsX69OZxvusnq44XMGWdYdtz778P334dhh3mbcuVMOUyfDh995Lc0Dt9JTYUHH7T+zW6OMVvCohhEpLmILBORlSLSO4P1HUUkWUR+9V6dA9Z1EJEV3qtDOOSJBXr3Nkfo00+HeaennWZfcJcRnS333GN+++7dXUZ0vmf8eLMYRo40U9KRJSErBhEpCLwIXA3UBtqISO0Mhk5U1Ybe6zVv27LAQOBcoAkwUETivu3K7NmWad+jR5iz7EuUME2zYIGZxI4sKVTIAlDWroVnnvFbGodv/P23hX2fdx60beu3NHFBOCyGJsBKVV2tqoeACUDLILf9P2CKqm5X1R3AFCAnkf4xh6pNYZ56qj3gh52bb7bm5P37w+7dEThA3uKyyywi8cknrQeGIx/y1FOwaZNF9RVws+fBEI6rVBn4M+D9em9Zem4Skd9E5EMROS2H28YN778PP/9sN6ITTojAAUTsC75li1VgdWTLyJHmt+/f329JHFHnjz/Mym7TxiwGR1BES31+CiSoan3MKngrpzsQkS4ikiQiScnJyWEXMBzs22dl3Rs1inBZ98aNzSQePdrmSRxZcsYZVvngzTctfNWRj+jTx/66h6gcEQ7FsAE4LeB9FW/ZUVR1m6qmtSR7DWgU7LYB+xinqomqmlihQoUwiB1+xoyxaKSoWKxPPmkHich8Vd6jXz+rZN69uwtfzTf8/LNVVezeHU4/3W9p4opw3L7mADVEpJqIFAFuAz4JHCAigaVBWwBLvf+/Bq4SkTKe0/kqb1ncsWWLTVm0amV9nCNOlSqW0zBxonm7HVlSurT1bZg+HT77zG9pHBFH1RRCxJx9eZuQFYOqHgEewG7oS4FJqrpYRAaLSAtv2EMislhEFgAPAR29bbcDQzDlMgcY7C2LO554wqaSomqx9uwJp5xif91jcLbccw/UrGnRYocP+y2NI6J8/DH8+KMVyYuIsy9v40pihIFly6BOHSvY+MILUT74K6/YgSdPhpbBBoPlXz75xC7T2LF22Rx5kMOHrQ5KwYLw228Wt+wAgi+J4RRDGGjVyqYoVq2CqLs/jhz5pxjQokXuR5ANqnDxxdbtbeVK9zCZJxk71oplffIJXH+939LEFK5WUpT44QezWnv39kEpgCmCkSPNbHn9dR8EiC9ELKx982YL6nLkMf7+25xJF18M113ntzRxi1MMIaBq4amVKllSm2+0aAEXXmjlhPfs8VGQ+KBpU7jxRhg1yoIGHHmIp5+2D3XUKHsKcOQKpxhC4NNP4aef7H4cVLvOSBH4GPzssz4KEj8MH271k4YM8VsSR9jYvNlqn7RuDU2a+C1NXOMUQy5JSYG+feGss+Cuu/yWBnsMbtnSnpS2bvVbmpinZk24+254+WXzDTnyAMOGWROUYcP8liTucYohl7z9NixebN/BmPH3DhtmU0kuyzMoBgywJkoDB/otiSNk1qwxLd+pkz2tOULCKYZccOCA3UwaN46x7oB16lgtjhdegD//zH58PqdiRSuV8d571tDLEccMHGjhqQMG+C1JnsAphlzwyitWm+vJJ2PQvzVokHnFn3jCb0nigl69rDx/v35+S+LINQsXwjvvmJZ3bW/DglMMOWTPHpuxufxye8UcVataDPd//gO//+63NDFPmTLWGC8tkMARh/TrZ9q9Vy+/JckzOMWQQ557DpKTY9y/1bcvFC/uJs+DpFs3qyzSt6+rLBJ3zJ5tWr1nT6uS6AgLTjHkgB07LOinRQs491y/pcmCChUssWLSJFdnOghKlrQ2wN9+C1On+i2NI0c8/rh937t181uSPIVTDDlg1ChrmhYXse89elhJUeeMC4rOna0yc//+zmqIG2bONE3ep4+rbRJmnGIIks2bbRrpttugfn2/pQmC0qVNOXz6qSvLHQRFi9rM2y+/2CVzxDiq5luoVMlVQ4wATjEEyZNPwsGDFvQTN3TrZma262npgBt1AAAgAElEQVQZFO3bQ/XqNjuRmuq3NI4s+eorixZ4/HHzpznCilMMQbBhg+XOtG8fZ7kzJ5xgZva0aTBjht/SxDyFClmU72+/wYcf+i2NI1NUTSFUqxYjZQfyHk4xBMHw4VYC4/HH/ZYkF9x7r5nbAwe6yfMguPVWyxMcMMAqmjtikI8/hrlz7UMqUsRvafIkYVEMItJcRJaJyEoROa6Pnog8KiJLROQ3EZkmIlUD1qWIyK/e65P02/rNunXw6quWaV+tmt/S5ILixS0O8/vvzXJwZEnBgmY1LFsG77/vtzSO40hNtYecGjWgbVu/pcmzhNyoR0QKAsuBK4H1WIvONqq6JGDMZcDPqrpPRLoCl6rqrd66Paqao5CCaDbquftuGD/emrqcdlpUDhl+Dh60H1KVKtbuMObStWOL1FT4179g715YujSGamE5bI7v5pst0/mOO/yWJu6IZqOeJsBKVV2tqoeACcAxPSZVdYaq7vPezgaqhOG4EWfVKksgvueeOFYKYCE3/fvDrFnmtHNkSYECZjWsXGn3H0eMkJJi1sLZZ1t4oCNihEMxVAYCK7at95ZlRifgy4D3xUQkSURmi0irzDYSkS7euKTk5OTQJA6SoUOt+mafPlE5XGTp2BESEmxe1vkasqVFC7MahgyxFsKOGGDSJFiyxEIDCxb0W5o8TVSdzyLSFkgERgUsruqZNrcDz4rImRltq6rjVDVRVRMrRKGH5sqVVlq7a1erwhn3FCli3vOkJPjsM7+liXlEzGpYvdqmEh0+k5JiH0jdujaV5Igo4VAMG4DAiZYq3rJjEJErgH5AC1U9mLZcVTd4f1cDM4FzwiBTyAwZYvfSxx7zW5Iw0r49nHnmPxVYHVly7bVWWn3oUDh0yG9p8jkTJlhEwMCBNtfniCjhuMJzgBoiUk1EigC3AcdEF4nIOcArmFLYErC8jIgU9f4vD1wALMFnVqywueWuXeHUU/2WJowUKmS+hnnznNUQBGlWw9q18NZbfkuTj0lJgcGDoV49a9btiDghKwZVPQI8AHwNLAUmqepiERksIi28YaOAE4AP0oWl1gKSRGQBMAMYERjN5BdDhpi/Nk9ZC2m0beushhzQvLm1Dx42zFkNvjFhAixf7qyFKBJyuKofRDJcdflyqFULHnkEnn46IofwnzffhDvvtEShFi2yHZ7f+fJLuOYaGDfOwpcdUeTIEcs4LFYM5s93iiFEohmumqcYOjQPWwtpOKshRzirwUecteAL7koHsHw5vPuuNUA7+WS/pYkgab6G+fPhk5hLNo85REyHrlvnIpSiypEjNq9bvz60yjSS3REBnGIIYNgwsxZ69vRbkiiQZjUMHuyshiBwVoMPTJzorAWfcFfbY+VKsxbuvdfaPOZ5ChWyevbz5sHnn/stTcyTZjWsXeushqiQkmLWQr16zlrwAacYPIYNsyznPO1bSE/btlYZ8IknnNUQBM2bW17D8OEuGzriTJpkeQsDBjhrwQfcFcdqIr39ttVEylN5C9lRuLBVXk1KstAbR5aI2H1qzRpXQymipFkLdeq4vAWfcIoB685WqFA+sxbSaN8eqlZ1voYgufZaq6E0bJjr1xAx/vtfK2v7+OPOWvCJfH/V07Jau3Sxfjb5jiJFzGr4+Wf45hu/pYl50qyGVavgvff8liYPkppq1kKtWtC6td/S5FvyvWIYMcIeSvKltZBGx45w+unO1xAkLVpAgwaW85KS4rc0eYyPPoJFiyyc2lVQ9Y18rRj+/BPeeMO6s1WJiw4REaJIEejd2/o1TJ/utzQxT5rVsGKF5V85wkRqqk1pnnWW9Vh1+Ea+VgwjR9rf3sc1I82H3HUXVK5sP0xHtrRqZRWghw1zVkPY+PRT+O03Zy3EAPlWMWzYYL2c77zTZlHyPUWLQq9e8N138O23fksT8xQoYL7RpUvNV+oIEVV7KDnzTGjTxm9p8j35VjGMGmWWa57ozhYuOne2eN0hQ/yWJC646SbzkQ4dat8lRwh8+aUlW/br55psxwD5UjFs2gSvvALt2lm3S4dH8eLmhZ82DX780W9pYp6CBW3WY+FCK1TryCVp1kJCgiVdOnwnXyqGZ56xejd9+/otSQxyzz1QoYLzNQTJrbdCjRpmZLmArlwyZYqFS/fpY0mXDt/Jd4ohORleegluvx2qV/dbmhikRAno0cNyGn75xW9pYp6CBW32Y/581xQvV6RZC6edBh06+C2NwyMsikFEmovIMhFZKSLHxfiISFERmeit/1lEEgLW9fGWLxOR/wuHPFkxejTs328/ZkcmdO0KZcs6X0OQ3HEHnHGGsxpyxcyZNm3Zq5cFQDhigpAVg4gUBF4ErgZqA21EpHa6YZ2AHapaHRgDjPS2rY31iK4DNAde8vYXEbZvhxdegFtugbPPjtRR8gAnngiPPmqPwPPn+y1NzFOokM2CzJkDX3/ttzRxxpAhULGiJRM5YoZwWAxNgJWqulpVDwETgJbpxrQE0tqpfwhcLiLiLZ+gqgdVdQ2w0ttfRHj2WdizxxyGjmx44AEoXdpZDUHSvr2FPbuSUznghx9gxgwLeChWzG9pYp7ff4frroPVqyN/rHAohsrAnwHv13vLMhyjqkeAXUC5ILcFQES6iEiSiCQlJyfnStAtW+Dmmy0xyZENJ50E3bpZiYKFC/2WJuZxyeO5YMgQa5XYpYvfksQFw4aZHj3xxMgfK26cz6o6TlUTVTWxQoUKudrHyy/D+++HWbC8TLdu9i0cOtRvSeKCtORxZ2QFQVrRxh49LODBkSUrV1rRxq5dLWgw0oRDMWwATgt4X8VbluEYESkEnARsC3LbsOIy7XNAmTLw4IPwwQeW4uvIkqJFbVbk228tgdyRBUOGQLlydqdzZMvw4WaV9ugRneOFQzHMAWqISDURKYI5k9N3mP8ESItFaw1MV1X1lt/mRS1VA2oALkYylnjkEXuiGzbMb0nigrvvdsnj2TJ3rrWT7d4dTjjBb2linjVrrJFYly7RayQWsmLwfAYPAF8DS4FJqrpYRAaLSAtv2OtAORFZCTwK9Pa2XQxMApYAXwH3q6orSRZLlC8P991nc3ArVvgtTcxTvDj07AlTp8JPP/ktTYwyZIhZo/ff77ckcYEfrQFE4zCEIjExUZOSkvwWI/+webP1hr71VvjPf/yWJubZu9eqOyQmuo6px7FgATRsaL0/BgzwW5qY588/ra5g586WmBsqIjJXVROzGxc3zmeHj5xyipXKePvt6MTKxTklS9pc8FdfueTx4xg6FEqVgoce8luSuGDkSAt/7tUrusd1isERHD17WibXk0/6LUlccN99ljzuAroCWLzYapQ/9JDlyDiyJLA1QNWq0T22UwyO4KhUyTyrb74J69b5LU3Mc+KJ5rf/9FOXPH6UoUPNnHr4Yb8liQtGjbImUH60BnCKwRE8vXqZF2zECL8liQsefNAejF2hWixtd+JEy6gvV85vaWKetNYA7dubey/aOMXgCJ4qVSyL6/XXzSvmyJK05PHJk61jZb5m2DAL2Xr0Ub8liQueftrf1gBOMThyRppdm9Yw25El3bqZrzVf5zUsX25pu/ffH5203ThnyxYYO9aq9vrVGsApBkfOOP106NjRvGIbIpqknicoU8Z8rR9+CIsW+S2NTwwfbmnh3bv7LUlc8Mwz/rcGcIrBkXP69DGv2FNP+S1JXPDII5bgmy8jlFatgnfegXvvtbBnR5Zs3Qovvgi33QY1a/onh1MMjpxTrZp5xcaNg7/+8luamKdsWXNET5oES5b4LU2UGT7c2nX27Om3JHHBM8/Avn3w+OP+yuEUgyN39OsHhw87qyFIHn3USk7lK6th9Wp46y1LjqxY0W9pYp5t2/5pJFarlr+yOMXgyB1nngnt2lkt802b/JYm5ilf3iI1J0ywyM18wfDhlhQZzSI/cczo0VZOxW9rAZxicISCsxpyRPfuZjXkiwilNWvMWujSxZIjHVmyfTs8/zy0bg116vgtjVMMjlCoXh3atnVWQ5BUqGBWw/vv5wOrYfhwa34S7SI/ccqYMfD337FhLYBTDI5Q6d/fMnFGjfJbkrige3fL88rTVsPatVY65e67raWdI0u2b4fnnjNroV49v6UxnGJwhEaa1TB2rLMagiBfWA3DhlnpFGctBMXo0WYtDBzotyT/EJJiEJGyIjJFRFZ4f8tkMKahiMwSkcUi8puI3Bqw7k0RWSMiv3qvhqHI4/CJNKvB+RqCokePPGw1rFlj1kKXLlZCxZEl27aZtXDzzVC3rt/S/EOoFkNvYJqq1gCmee/Tsw9or6p1gObAsyISWHO3p6o29F6/hiiPww+qV7cIpbFjXV5DEARaDXmulfbQoeZb8KMkaBzyzDMWiRRL1gKErhhaAm95/78FtEo/QFWXq+oK7/+NwBbAFUzJa/TvbxFKroZSUPTsaRWon3jCb0nCyKpV/+QtuEikbNm61SKRbrklNiKRAglVMZyiqmmPiJuALHPeRaQJUARYFbB4mDfFNEZEioYoj8MvzjwTOnSwCKWNG/2WJuYpX/6fbOg8U0Np6FDLcu6d0cSBIz1p1kIsdjjNVjGIyFQRWZTBq2XgOLXm0Zk2kBaRisDbwJ2qmuot7gOcDTQGygKZeqtEpIuIJIlIUnJycvZn5og+/ftbDSXXryEoune3Gkp5wmpYudJav3bt6rKcg2DLFvj3v62Neu3afkuTAaqa6xewDKjo/V8RWJbJuFLAPKB1Fvu6FPgsmOM2atRIHTFK586qRYqo/vGH35LEBY8/rgqqCxb4LUmItGunWry46l9/+S1JXNC9u2qBAqq//x7d4wJJGsQ9NtSppE+ADt7/HYCP0w8QkSLAR8B4Vf0w3bqK3l/B/BN5xajOv/Tvb93Lhw3zW5K44JFHrKHPoEF+SxICS5daBdUHHoBTT/Vbmpjnr7+sgmrbtv5WUM2KUBXDCOBKEVkBXOG9R0QSReQ1b8wtwMVAxwzCUt8VkYXAQqA8kJ9KjOVNqla1xKbXX7fQRUeWlCljyuGjj2DuXL+lySWDBpkn3dVECoonn7Q4jVj0LaQhZl3EF4mJiZqUlOS3GI7M2LjRnNG33Qb/+Y/f0sQ8u3dbJfNzz4UvvvBbmhyyYAE0bGiWYp5MzAgvf/75T3T3a69lPz7ciMhcVU3MbpzLfHaEn0qVzAk5fry1dXRkSalSliT85Zfw449+S5NDBg60uTDXyzkohg2zmdZYqYmUGU4xOCJD795QrFicT55Hj/vvtwZnsX7DOIY5c+Djjy28qsxxRQ8c6Vi92mZYO3e2GddYxikGR2Q4+WRrdjxhAixc6Lc0MU/JktC3L8yYAdOn+y1NkDz+OJQrB926+S1JXDBokLWn6N/fb0myxykGR+To2dPmSeLhlxADpJUX6tfPphtimm+/ha+/ttIXpUr5LU3Ms3ixBW49+GB8JIU7xeCIHGXLmnL45BOYPdtvaWKeYsUsUmX2bPj0U7+lyQJVM28qVYL77vNbmrhgwABLZoyXgrNOMTgiS7duNq3Ur5/fksQFHTtCjRp2uVJS/JYmE774An76ye52xYv7LU3Mk5QE//ufuWLKlfNbmuBwisERWU44we5y06fD1Kl+SxPzFC5sUZ+LFln11ZgjNdU+zzPPhLvu8luauKBfP1MIjzzityTB4xSDI/Lccw+cfrpNP8T85Ln/3HyzpQYMGGBtLmKKSZMsd2HwYNNijiyZPh2++Sb+XDFOMTgiT9GiFpIxZ46l+DqypEABa5m8Zg28+qrf0gRw+LAFEtSrZ8mLjixRtajt006zcOR4wikGR3Ro187KSPbpA0eO+C1NzNO8OVx0kU0r7dnjtzQer75qPRdGjDDt5ciS//7XnoWeeMICC+IJ9+k6okOhQlYkZvlyeOMNv6WJeUSs59HmzTBmjN/SYNrpiSfgkkvg6qv9libmOXLEfAu1a0P79n5Lk3OcYnBEj+uvhwsusGmlvXv9libmadoUbrjBWmlv2eKzMKNHmxAjR5rWcmTJG2/YM9Dw4dbpNN5wisERPdIeg//6yzqgO7LlySdh/36f69Nt2QKjRsFNN1mlP0eW7N1rzz5Nm0KLFn5LkzucYnBElwsugJYtTUFs3eq3NDFPzZpWW+fll61Jmi8MGWLayfXYCIrRo+3ZZ9So+DWunGJwRJ/hw23OevBgvyWJCwYOhCJFfMoRXL7ctFLnzrHbVSaG2LzZpv5uuMGegeIVpxgc0ad2bWvmM3asK8sdBBUrWtbspEk+VBZJq5KbJxpTR55Bg+DAgfhvex6SYhCRsiIyRURWeH8zrL0rIikB3ds+CVheTUR+FpGVIjLRawPqyA+kxfDFS/EYn+nZ08pyP/poFHMEv/vO8k5697aDO7Jk6VKL6L33XjjrLL+lCY1QLYbewDRVrQFM895nxH5Vbei9At0xI4Exqlod2AF0ClEeR7xwyil2w5k82W5Ajiw58UQYOhRmzYIPP8x+fMikpkKPHlC5cnzVcvCRXr2sfHost+wMllAVQ0vgLe//t4BWwW4oIgI0A9K+5jna3pEHeOQRu/H06GE3IkeW3HmnJR336gUHD0b4YBMnWnbWsGFQokSEDxb/TJtmFXH79IEKFfyWJnRCVQynqOpf3v+bgMzszWIikiQis0Uk7eZfDtipqmlpsOuByiHK44gnSpSwG8+cOfDee35LE/MULGgRL2vWwPPPR/BA+/ebNdewIbRtG8ED5Q1SUuwZJyEBHn7Yb2nCQ7aKQUSmisiiDF4tA8epqgKZzX5W9RpQ3w48KyJn5lRQEeniKZek5OTknG7uiFXatYNGjewxOGZqP8QuV1wB11xj00oR+xk8/TT88YelXMdjdlaUef11a1I4alT8lb7IFFXN9QtYBlT0/q8ILAtimzeB1oAAW4FC3vKmwNfBHLdRo0bqyEP8+KMqqPbv77ckccGSJaqFCql26RKBnf/xh2rx4qqtW0dg53mPnTtVK1RQvegi1dRUv6XJHiBJg7jHhjqV9AnQwfu/A/Bx+gEiUkZEinr/lwcuAJZ4Qs7wlESm2zvyAeefD7ffbo9ca9f6LU3MU6sWPPCARcDMnx/mnffubf6eUaPCvOO8ydChlqf57LPxm8yWEaEqhhHAlSKyArjCe4+IJIrIa96YWkCSiCzAFMEIVV3iresFPCoiKzGfw+shyuOIV9Iqdj72mN+SxAUDB0L58vDQQ2EMX/3pJ/P19OhhE+aOLFm+3Cq7dOwI//qX39KEF9E4bJySmJioSUlJfovhCDeDB9sdb8YMuPRSv6WJeV57zfIE338/DO0RUlLgvPNg40ZYtsw67zkyRdWKzM6aZQoiXtI8RGSumr83S1zmsyN26NnTnlQfeMCawjiy5M477Um1Z88wFKt9/XVrTvzUU04pBMHHH8PXX9uzTLwohZzgFIMjdihe3GzzxYsjHI+ZNyhY0C7T+vU2151rtm61APxLLjFfjyNL9u+3sNS6deOvM1uwOMXgiC2uvx6uvdamlDZs8FuamOf8881yePppWLIk+/EZ0qcP7NoFL76YtzyoEWLECFi3Dl54wfpP5UWcYnDEFiJmNRw+bE5QR7aMHGklM+6/PxeO6NmzzVnx8MNQp05E5MtLrFxp17tNGzOw8ipOMThijzPPtLDJCROs1oAjSypUsKfYmTNzmECekmLapFIls9AcWaIK991nJdCfftpvaSKLUwyO2KRXL1MQ995rk7qOLOncGZo0sfLcO3cGudHzz8O8eVZn48QTIypfXuD992HKFOuqV6mS39JEFqcYHLFJ8eL/tC1zncOypUABu1zJyWZsZcsff0D//lZf45ZbIi5fvLNjh9VDatLEnlXyOk4xOGKXK66wWkojR8KiRX5LE/Occ471a3jllWwqmav+45BwDueg6N0btm2za5sfykc5xeCIbZ55Bk46Cbp0caW5g+CJJ6BaNUt8O3Agk0H//S989pn1cnYZztny3Xcwbpz55xs29Fua6OAUgyO2qVDB5sBnzbJWoI4sKVHCnmqXL88kt2HHDnjwQcuMe+ihqMsXb+zbB506mbLNT91NnWJwxD7t2sGVV5pDes0av6WJea68Ejp0sBm4BQvSrezWzRwRr72Wd4Pww8iAAebmev11686WX3CKwRH7iNiNrEABuOsuN6UUBM88A+XKmYI4dMhb+Omn8Pbb0LevOSQcWTJ7trWkuPdeuOwyv6WJLk4xOOKD00+3u93MmRZ+48iScuVsSmnBAm9Kaft289PUr2/RSI4sOXDAnkEqVzbLK7/hFIMjfujcGa66ykpzr17ttzQxT8uW0L49DB8OSW2ftZpIb75pGVqOLOnfH5YuNadzqVJ+SxN9nGJwxA8i1p2mQAGbI0lJ8VuimOe55+DUk/bR/svbOPDYADeFFAQzZli8Q9eu0Ly539L4g1MMjvji9NMt9v6HH6wOhCNLSu9Zz+uH2rOU2vTZ09dvcWKenTvtmaN69fzdxC6ksAQRKQtMBBKAtcAtqroj3ZjLgDEBi84GblPVySLyJnAJsMtb11FVf82NLIcPH2b9+vUcyDR42xFuihUrRpUqVShcuHB0D9y2LXzxhdX3ufJKS0d1HE9qKnTowP+lzuaBtjt59t+luaq5NZhxZMz991uvop9+yl9RSMcRTGPozF7AU0Bv7//ewMhsxpcFtgMlvPdvAq1zetxGjRod1+R69erVmpycrKnx0JE7D5CamqrJycm6evVqfwTYsUP19NNVq1dX/ftvf2SIdUaNUgXV117T/ftV69VTPflk1U2b/BYsNnnnHbtcTzzhtySRA0jSIO6xoU4ltQTe8v5/C2iVzfjWwJequi/E4x7HgQMHKFeuHOLS+6OCiFCuXDn/LLTSpS30ctUq6/jmOJakJAtLvfFGuOsuihWzYrW7d9tUiYv4PZZly+Cee+DCC+2y5XdCVQynqOpf3v+bgOya3N0GvJ9u2TAR+U1ExohI0cw2FJEuIpIkIknJycmZjQlWbkcY8P16X3wxPP44vPUWvPGGv7LEEjt2wM03Q8WKFlbjfU61a1tc/tdf5/2y0Tlh/367XMWLWwVVl/cXhGIQkakisiiDV8vAcZ6ZkmmbEBGpCNQDvg5Y3AfzOTTGppl6Zba9qo5T1URVTaxQoUJ2YkednTt38tJLL0XlWDNnzuSnn37KdP3kyZMZPHhw2I532223sWLFirDtL6wMGACXX26Tw8el+eZDUlMtRnXDBpg0yRIaArjnHmjd2pq2zZzpj4ixRrdusHChGaBVqvgtTYwQzHxTZi9gGVDR+78isCyLsd2AcVmsvxT4LJjjZuRjWLJkSWiTbyGyZs0arVOnTo62SU1N1ZSUlBwfa+DAgTpq1KhM1zdt2lSTk5NzvN/MmDlzpnbu3DnDdX5fd1VV3bxZtVIl8zfs3Om3NP4yYoRNlP/735kO2b1btWZN8zesXx9F2WKQ8ePtcvXu7bck0YEgfQyhKoZRHOt8fiqLsbOBy9ItS1MqAjwLjAjmuLGoGG699VYtVqyYNmjQQHv06KF///23NmvWTM855xytW7euTp48WVVNgZx11lnarl07rV27tq5du1Zfe+01rVGjhjZu3Fg7d+6s999/v6qqbtmyRW+88UZNTEzUxMRE/eGHH3TNmjV6yimnaKVKlbRBgwb63XffHSPHsmXL9NJLLz36ftOmTdqqVSutX7++1q9fX3/88UdVVX3mmWe0Tp06WqdOHR0zZoyqqu7Zs0evueYarV+/vtapU0cnTJigqqopKSmakJCghw8fPu68/b7uR/n+e9WCBVVbtVLNhbLNE0ybZtfg5ptVswnCWLxYtWRJ1fPPVz14MEryxRhz5qgWLap6ySWqGXy18yTRUgzlgGnACmAqUNZbngi8FjAuAdgAFEi3/XRgIbAIeAc4IZjjZqsYunWzTzucr27dsrzg6S2Gw4cP665du1RVNTk5Wc8880xNTU3VNWvWqIjorFmzVFV1w4YNWrVqVd22bZseOnRIL7zwwqOKoU2bNvr999+rquq6dev07LPPVtWsLYY33nhDH3300aPvb7nllqM3/iNHjujOnTs1KSlJ69atq3v27NG///5ba9eurfPmzdMPP/zwGMtgZ8DT9xVXXKFJSUnHHS9mFIOq6pgx9pV+/HG/JYk+K1aolimjWru2qve9y46JE+1yde0aYdlikE2bVKtUUa1aVXXLFr+liR7BKoaQ3Cyqug24PIPlSUDngPdrgcoZjGsWyvFjGVWlb9++fPfddxQoUIANGzawefNmAKpWrcp5550HwC+//MIll1xC2bJlAbj55ptZvnw5AFOnTmXJkiVH97l792727NmT5XH/+usvAn0w06dPZ/z48QAULFiQk046iR9++IEbbriBkl6g9o033sj3339P8+bN6d69O7169eK6667joosuOrqfk08+mY0bN9KoUaNQL03kSJssHjLEGtvfeqvfEkWHXbugRQtzMn/ySdA1HG65xYKXRo2CWrWsGnd+4NAhuOkma7zz009W2d1xLHnT//7ss35LwLvvvktycjJz586lcOHCJCQkHA3tLBlk5kxqaiqzZ8+mWLFiQR+3ePHi7Nq1K/uBGXDWWWcxb948vvjiC/r378/ll1/OgAEDAAsHLl68eK72GzVE4KWXrBlBx47WMzox0W+pIktKCrRpAytWwDff2DnngCeftMv18MOW7ZvXk99UzQH/448WvptfGu/kFFcSI0yceOKJ/P3330ff79q1i5NPPpnChQszY8YM1q1bl+F2jRs35ttvv2XHjh0cOXKE//73v0fXXXXVVTz//PNH3//6668ZHiuQWrVqsXLlyqPvL7/8csZ6DW5SUlLYtWsXF110EZMnT2bfvn3s3buXjz76iIsuuoiNGzdSokQJ2rZtS8+ePZk3b97R/Sxfvpy6devm4spEmaJFrUPZKafA9dfn7WJ7qvaY/+WX8Hi+QPkAAA5zSURBVPzzuaoNXbAgvPOOFV299da830F14ECrIzhoUP4xKHNFMPNNsfaKReezqvkE6tSpoz169NDk5GQ977zztG7dutqxY0c9++yzdc2aNRlGL73yyitavXp1bdKkibZv31779u2rquabuOWWW7RevXpaq1Ytveeee1TVHMz16tXL0Pm8d+9erV279tEM8E2bNmmLFi20bt262qBBA/3pp59UNWPn81dffXV0v4mJiTpnzpyj+2jcuHGG5xwL1z1DFi+2Offq1S1qKS/yxBPmJOjZM+Rd/fmnasWKqpUrq65ZE7poscgrr9jl6tQpW998noVoOJ/9esWqYsgtf3slHQ4fPqzXXXed/u9//wtpfw899JBOmTIlHKKpquro0aP1tddey3BdTF/3n35SLV5ctVEji9HMS7z8sv18O3QI211uwQLV0qVNl+a1shkff6xaoIDq1VerHjrktzT+EaxicFNJMcCgQYNo2LAhdevWpVq1arRqlV1lkazp27cv+/aFr+pI6dKl6dChQ9j2FzWaNrUkr19/hVatrIFvXmDSJLjvPrjmGitDHqYM9Pr1rTbhxo3wf/9nlUbzAl99ZZnNjRrZpYt2zce4JBjtEWuvvGYxxDNxcd3Hj1cVUW3WTHXvXr+lCY3337dchQsvVN2zJyKH+Ppr1cKFVc8912oVxjPffGO5Cueco7p9u9/S+A/OYnA4PNq1s3pKM2bAddfB3r1+S5Q73nsP7rgDLrjAHM4Rqgt91VXwwQcwbx40a2aN3+KRadMsirdmTZgyBcqU8Vui+MEpBkf+oF07GD8evv3WpmDibZ7kP/+xc7j4YpvvOeGEiB6uZUv4+GNrb3nZZbBpU0QPF3Y+/NA+5jPPhKlTjysZ5cgGpxgc+Ye2beHdd2HWLKuv/McffkuUPaoWW3nXXVYs8LPPotZB5uqr4fPPLeL3wgvh99+jctiQeeklS95LTITvvnMJbLnBKQZH/uK228wb+eefcN555piOVQ4fNoXwxBOWsPf551FvK9asmU3J7N5tvvxp06J6+ByRkmJVY++/H6691qaPvIICjhziFEMYOf/888O+z7Vr1/Lee+9luv6vv/7iuuuuy3IfwcgV06W1w02zZpb6WrCgPQpncX19Y+NGsxDefNOyst54w7dwmvPOg19+gcqVoXlzePllM2RiiW3bTBmMGAFdusBHH0GJEn5LFccE46GOtVd+ikqaMWOGXnvttZmu79Gjx9HKraGQVWntrIjr675hg0X3gOo996ju3++3RMaUKaoVKqiWKGH9JmOEnTtVmze3y3XrrbFT4XzePNWEBNUiRVTHjfNbmtgGl+AWfUqWLKmqdjO/5JJL9KabbtKaNWvq7bfffjQTuWrVqtqzZ0+tW7euNm7cWFesWKGqqh06dNAPPvjguH2de+65WqpUKW3QoIGOHj36uGNWq1ZNDxw4oKqqixYt0saNG2uDBg20Xr16unz58qDlyqq0dlbEwnUPiUOHVB97zH4KDRqozp/vnyz796v26WOhtbVrW/Z2jHHkiOrw4RYxm5BgOYR+ceiQ6uDBFlpbubLq7Nn+yRIvBKsY8mQRvYcfDv/UccOGOavNN3/+fBYvXkylSpW44IIL+PHHH7nwwgsBOOmkk1i4cCHjx4/n4Ycf5rPPPst0PyNGjODpp5/OcMyaNWsoU6YMRYtaR9SXX36Zbt26cccdd3Do0CFSUlKClqtAgQJUr16dBQsWxHYF1XBTuDCMHGlTSp07m8eye3ebvonmXMSMGVbdbcUK8yv8+99R9ycEQ8GCNo9/6aVWu++CC6BrVxg6NLrhoAsWwJ13wvz5Jsfzz7vIo3DifAwRokmTJlSpUoUCBQrQsGFD1q5de3RdmzZtjv6dNWtWro+RvsR206ZNGT58OCNHjmTdunUZVkPNSq600tr5kuuvt9jMDh3gqaegXj3zPWSgXMPKypUWLdWsmR1ryhR4/fWYVAqBNG1qN+cHHzSfQ82a5gY5ciSyx12/3vTmv/5l3Uv/9z/7mJxSCC950mKIgarbR5/iwfogHAn4xUhACYO0/wsVKkRqaipg5bYPHTqU7TGKFy9+tJQ3wO233865557L559/zjXXXMMrr7xCs2bHtrzISq64KK0dScqWtZty27bW2+GOO2DYMIsKatUqvF3iV6+G4cPNuVykiD2G9+8fVx7Tk06C556zJ/f77oNOnexy9epl+jXgqxYya9fCCy/Aiy9aW+uHH4Z+/VzUUaQIyWIQkZtFZLGIpIpIpoXvRaS5iCwTkZUi0jtgeTUR+dlbPlFEioQiT7wwceLEo3+bNm0KQEJCAnPnzgXgk08+4fDhw0DWJbbPOuusY574V69ezRlnnMFDDz1Ey5Yt+e2333IkV9yU1o40l11mc5ETJ9pd6OaboWpVuxOtWpX7/R44AO+/D1deac0P3n7bYivTlEQcKYVAGjaEH36AyZPtyf2eeyAhAXr0sOzp3EYwHThgEbotWsAZZ8CYMdC6NSxbBs8845RCJAl1KmkRcCPwXWYDRKQg8CJwNVAbaCMitb3VI4Exqlod2AF0ClGeuGDHjh3Ur1+f5557jjFjxgBw99138+2339KgQQNmzZp1tJlP/fr1KViwIA0aNDg6No2SJUty5plnHu2/MGnSJOrWrUvDhg1ZtGgR7du3D1qmzZs3U7x4cU499dQwnWWcU6CAZUktWmTzFeecY7GQ1atbu7P777f02iVLMi7Ol5pqcx3ffWfbXXWV3cluv92mj554whTCc89BHrjmBQpYtvTPP1u/oCZNzE3SqJFNM3XpYsbR0qUZXy5V2LIFZs40f8H119vluu46mD3bDKo1a0yXJiRE+eTyIaJhCEgWkZlAD7WWnunXNQUGqer/ee/7eKtGAMnAqap6JP24rEhMTNSkpGMPtXTpUmrVqhXaiUSBhIQEkpKSKF++fFj299FHHzF37lyGDh0a0n7GjBlDqVKl6NQpZ7o5Xq57WNiwwayIadPshh/YZvXkk/954k9Nhc2b4eDBf9bXrWt+hBYtzCIpkPfde9u3m+6cPNmSzQOrkJQqZRnJqalmGezZA4GGcbVqlpdw9dWWzhHOaan8jIjMVdVs2xpGw8dQGfgz4P164FygHLBTVY8ELD+uL3QaItIF6AJw+umnR0bSOOSGG25g27ZtIe+ndOnStGvXLgwS5WEqV4ZHH7XX4cMWErNypT3Krlt3rCI45RS7u1WrZtbGKaf4J7dPlC1rlkKXLqYAfv8d5s41/bpxIyQnW1BYsWJQvLhNF9WubQZZ5cphqybuyAXZKgYRmQpkZOv2U9WPwy9SxqjqOGAcmMUQreOGm0CfQLjo3LlzyPu48847wyBJPqJwYZsvadLEb0niggIF7KZfu3b2Yx3+k61iUNUrQjzGBuC0gPdVvGXbgNIiUsizGtKWOxwOh8NHojHROQeo4UUgFQFuAz7xsvBmAK29cR2AkCyQcPhLHMHjrrfDkTcJNVz1BhFZDzQFPheRr73llUTkCwDPGngA+BpYCkxS1cXeLnoBj4rISszn8HpuZSlWrBjbtm1zN6sooaps27aNYsWK+S2Kw+EIM2GJSoo2GUUlHT58mPXr1x+T8OWILMWKFaNKlSoUdk10HY64IJaikqJC4cKFqVatmt9iOBwOR9yT94OpHQ6Hw5EjnGJwOBwOxzE4xeBwOByOY4hL57OIJAPrcrl5eWBrGMWJNvEuP8T/OcS7/BD/5xDv8oM/51BVVStkNyguFUMoiEhSMF75WCXe5Yf4P4d4lx/i/xziXX6I7XNwU0kOh8PhOAanGBwOh8NxDPlRMYzzW4AQiXf5If7PId7lh/g/h3iXH2L4HPKdj8HhcDgcWZMfLQaHw+FwZEG+UgyZ9Z6OB0TkDRHZIiKL/JYlN4jIaSIyQ0SWeH3Cu/ktU04RkWIi8ouILPDO4Qm/ZcoNIlJQROaLyGd+y5IbRGStiCwUkV9F5LiukbGOiJQWkQ9F5HcRWep1r4wp8s1Uktd7ejlwJdYtbg7QRlWX+CpYkIjIxcAeYLyq1vVbnpwiIhWBiqo6T0ROBOYCreLl+gOIiAAlVXWPiBQGfgC6qepsn0XLESLyKJAIlFLV6/yWJ6eI/H97d8waRRhFYfi9GguNoIUiQopY2ZogaSIiCqIo+ge0sBbBStDGfyB2NlEJGE1hkk5EQUEri4hgoYUEwYiygohiE9RjMV8gownZSSDfTOY8sOywsHAWlr0zd+/ujffAfkmN/B1DRIwCzyWNpFUEWyR9W+55a6lNVwxDwDtJM5LmgHHgdOZMXZP0DPiaO8dKSfok6WU6/kHxF+xLrnKtIxXmFz1vSrdGnVlFRB9wAhjJnaWNImIbcJC0YkDSXN2KArSrMCy2e7pRH0zrRUT0AwPAi7xJqkttmFdAB3gsqWmv4TpwCfiTO8gqCHgUEdNpF3yT7AG+ALdTO28kInpzh/pXmwqD1UBEbAUmgIuSvufOU5Wk35L2UayiHYqIxrT1IuIk0JE0nTvLKh2QNAgcB86nNmtT9ACDwA1JA8BPoHbfd7apMCy1e9rWSOrLTwBjkiZz51mNdPn/FDiWO0sFw8Cp1KMfBw5HxJ28kaqT9DHdd4ApijZxU8wCswuuNO9TFIpaaVNhWHT3dOZMrZG+uL0JvJF0LXeelYiInRGxPR1vphhkeJs3VfckXZbUJ6mf4v3/RNKZzLEqiYjeNLxAasEcBRozqSfpM/AhIvamh44AtRvAWDcb3JYj6VdEzO+e3gjcWrB7uvYi4h5wCNiR9mxflbTiHdkZDANngdepRw9wRdKDjJmq2g2Mpgm3DRT7yxs58tlgu4Cp4jyDHuCupId5I1V2ARhLJ6gzwLnMef7TmnFVMzPrTptaSWZm1gUXBjMzK3FhMDOzEhcGMzMrcWEwM7MSFwYzMytxYTAzsxIXBjMzK/kLQgzooQ43nFwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# torch.manual_seed(1) # reproducible\n",
    "# Hyper Parameters\n",
    "TIME_STEP = 10 # rnn time step\n",
    "INPUT_SIZE = 1 # rnn input size\n",
    "LR = 0.02 # learning rate\n",
    "# show data\n",
    "steps = np.linspace(0, np.pi*2, 100, dtype=np.float32)\n",
    "x_np = np.sin(steps) # float32 for converting torch FloatTensor\n",
    "y_np = np.cos(steps)\n",
    "plt.figure(1)\n",
    "plt.suptitle('Input(sin) and Target(cos)',fontsize='18')\n",
    "plt.plot(steps, y_np, 'r-', label='target (cos)')\n",
    "plt.plot(steps, x_np, 'b-', label='input (sin)')\n",
    "plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(1, 36, batch_first=True)\n",
      "  (out): Linear(in_features=36, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "h_size = 36#number of rnn hidden units\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "        input_size=INPUT_SIZE,\n",
    "        hidden_size=h_size, # rnn hidden unit\n",
    "        num_layers=1, # number of rnn layer\n",
    "        batch_first=True, # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(h_size, 1)\n",
    "    def forward(self, x, h_state):\n",
    "         # x (batch, time_step, input_size)\n",
    "         # h_state (n_layers, batch, hidden_size)\n",
    "         # r_out (batch, time_step, hidden_size)\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        outs = [] # save all predictions\n",
    "        print(r_out.size(1))\n",
    "        for time_step in range(r_out.size(1)): # calculate output for each time step\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "        return torch.stack(outs, dim=1), h_state\n",
    " # instead, for simplicity, you can replace above codes by follows\n",
    " # r_out = r_out.view(-1, 32)\n",
    " # outs = self.out(r_out)\n",
    " # return outs, h_state\n",
    "rnn = RNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor([[[-0.0960],\n",
      "         [-0.0689],\n",
      "         [-0.1205],\n",
      "         [-0.1434],\n",
      "         [-0.1502],\n",
      "         [-0.1480],\n",
      "         [-0.1381],\n",
      "         [-0.1226],\n",
      "         [-0.1035],\n",
      "         [-0.0831]]], grad_fn=<StackBackward>)\n",
      "tensor(0.5609, grad_fn=<MseLossBackward>)\n",
      "loss : 0.560881\n",
      "10\n",
      "tensor([[[0.1184],\n",
      "         [0.0820],\n",
      "         [0.0827],\n",
      "         [0.1007],\n",
      "         [0.1368],\n",
      "         [0.1710],\n",
      "         [0.1925],\n",
      "         [0.1953],\n",
      "         [0.1763],\n",
      "         [0.1348]]], grad_fn=<StackBackward>)\n",
      "tensor(0.5224, grad_fn=<MseLossBackward>)\n",
      "loss : 0.522380\n",
      "10\n",
      "tensor([[[-0.0061],\n",
      "         [ 0.0567],\n",
      "         [ 0.0757],\n",
      "         [ 0.0733],\n",
      "         [ 0.0173],\n",
      "         [-0.0524],\n",
      "         [-0.1090],\n",
      "         [-0.1310],\n",
      "         [-0.1069],\n",
      "         [-0.0288]]], grad_fn=<StackBackward>)\n",
      "tensor(0.4683, grad_fn=<MseLossBackward>)\n",
      "loss : 0.468252\n",
      "10\n",
      "tensor([[[0.0367],\n",
      "         [0.0359],\n",
      "         [0.1276],\n",
      "         [0.2986],\n",
      "         [0.5362],\n",
      "         [0.7843],\n",
      "         [0.9816],\n",
      "         [1.0954],\n",
      "         [1.1244],\n",
      "         [1.0724]]], grad_fn=<StackBackward>)\n",
      "tensor(0.4719, grad_fn=<MseLossBackward>)\n",
      "loss : 0.471939\n",
      "10\n",
      "tensor([[[ 0.7327],\n",
      "         [ 0.3751],\n",
      "         [-0.1253],\n",
      "         [-0.6786],\n",
      "         [-1.1068],\n",
      "         [-1.3360],\n",
      "         [-1.4284],\n",
      "         [-1.4567],\n",
      "         [-1.4514],\n",
      "         [-1.4200]]], grad_fn=<StackBackward>)\n",
      "tensor(0.7341, grad_fn=<MseLossBackward>)\n",
      "loss : 0.734130\n",
      "10\n",
      "tensor([[[-1.0091],\n",
      "         [-1.0992],\n",
      "         [-1.1079],\n",
      "         [-1.0916],\n",
      "         [-1.0546],\n",
      "         [-1.0111],\n",
      "         [-0.9699],\n",
      "         [-0.9442],\n",
      "         [-0.9429],\n",
      "         [-0.9669]]], grad_fn=<StackBackward>)\n",
      "tensor(1.5171, grad_fn=<MseLossBackward>)\n",
      "loss : 1.517076\n",
      "10\n",
      "tensor([[[-0.6486],\n",
      "         [-0.5822],\n",
      "         [-0.5255],\n",
      "         [-0.5166],\n",
      "         [-0.5315],\n",
      "         [-0.5539],\n",
      "         [-0.5737],\n",
      "         [-0.5898],\n",
      "         [-0.5993],\n",
      "         [-0.5983]]], grad_fn=<StackBackward>)\n",
      "tensor(0.8691, grad_fn=<MseLossBackward>)\n",
      "loss : 0.869112\n",
      "10\n",
      "tensor([[[-0.1772],\n",
      "         [-0.2089],\n",
      "         [-0.1575],\n",
      "         [-0.1152],\n",
      "         [-0.0725],\n",
      "         [-0.0352],\n",
      "         [ 0.0032],\n",
      "         [ 0.0352],\n",
      "         [ 0.0577],\n",
      "         [ 0.0659]]], grad_fn=<StackBackward>)\n",
      "tensor(0.4215, grad_fn=<MseLossBackward>)\n",
      "loss : 0.421477\n",
      "10\n",
      "tensor([[[0.0721],\n",
      "         [0.1345],\n",
      "         [0.1756],\n",
      "         [0.1932],\n",
      "         [0.1930],\n",
      "         [0.1805],\n",
      "         [0.1587],\n",
      "         [0.1280],\n",
      "         [0.0905],\n",
      "         [0.0506]]], grad_fn=<StackBackward>)\n",
      "tensor(0.5476, grad_fn=<MseLossBackward>)\n",
      "loss : 0.547614\n",
      "10\n",
      "tensor([[[ 0.0990],\n",
      "         [ 0.0103],\n",
      "         [-0.0254],\n",
      "         [-0.0553],\n",
      "         [-0.0632],\n",
      "         [-0.0524],\n",
      "         [-0.0224],\n",
      "         [ 0.0222],\n",
      "         [ 0.0768],\n",
      "         [ 0.1338]]], grad_fn=<StackBackward>)\n",
      "tensor(0.5241, grad_fn=<MseLossBackward>)\n",
      "loss : 0.524085\n",
      "10\n",
      "tensor([[[0.1153],\n",
      "         [0.1804],\n",
      "         [0.2343],\n",
      "         [0.2700],\n",
      "         [0.2735],\n",
      "         [0.2524],\n",
      "         [0.2158],\n",
      "         [0.1666],\n",
      "         [0.1076],\n",
      "         [0.0447]]], grad_fn=<StackBackward>)\n",
      "tensor(0.5457, grad_fn=<MseLossBackward>)\n",
      "loss : 0.545703\n",
      "10\n",
      "tensor([[[ 0.0302],\n",
      "         [-0.0463],\n",
      "         [-0.0867],\n",
      "         [-0.1159],\n",
      "         [-0.1055],\n",
      "         [-0.0730],\n",
      "         [-0.0276],\n",
      "         [ 0.0271],\n",
      "         [ 0.0879],\n",
      "         [ 0.1472]]], grad_fn=<StackBackward>)\n",
      "tensor(0.4811, grad_fn=<MseLossBackward>)\n",
      "loss : 0.481109\n",
      "10\n",
      "tensor([[[ 0.0796],\n",
      "         [ 0.1295],\n",
      "         [ 0.1905],\n",
      "         [ 0.2226],\n",
      "         [ 0.1941],\n",
      "         [ 0.1433],\n",
      "         [ 0.0958],\n",
      "         [ 0.0458],\n",
      "         [-0.0117],\n",
      "         [-0.0685]]], grad_fn=<StackBackward>)\n",
      "tensor(0.4756, grad_fn=<MseLossBackward>)\n",
      "loss : 0.475577\n",
      "10\n",
      "tensor([[[-0.0554],\n",
      "         [-0.1096],\n",
      "         [-0.1495],\n",
      "         [-0.1630],\n",
      "         [-0.1115],\n",
      "         [-0.0484],\n",
      "         [ 0.0024],\n",
      "         [ 0.0510],\n",
      "         [ 0.1036],\n",
      "         [ 0.1491]]], grad_fn=<StackBackward>)\n",
      "tensor(0.4310, grad_fn=<MseLossBackward>)\n",
      "loss : 0.431009\n",
      "10\n",
      "tensor([[[ 0.0380],\n",
      "         [ 0.0662],\n",
      "         [ 0.1314],\n",
      "         [ 0.1418],\n",
      "         [ 0.0575],\n",
      "         [-0.0278],\n",
      "         [-0.0754],\n",
      "         [-0.1181],\n",
      "         [-0.1701],\n",
      "         [-0.2094]]], grad_fn=<StackBackward>)\n",
      "tensor(0.4072, grad_fn=<MseLossBackward>)\n",
      "loss : 0.407162\n",
      "10\n",
      "tensor([[[-0.1407],\n",
      "         [-0.1654],\n",
      "         [-0.2058],\n",
      "         [-0.1866],\n",
      "         [-0.0672],\n",
      "         [ 0.0386],\n",
      "         [ 0.0908],\n",
      "         [ 0.1293],\n",
      "         [ 0.1722],\n",
      "         [ 0.1939]]], grad_fn=<StackBackward>)\n",
      "tensor(0.3591, grad_fn=<MseLossBackward>)\n",
      "loss : 0.359145\n",
      "10\n",
      "tensor([[[ 0.0369],\n",
      "         [ 0.0296],\n",
      "         [ 0.0811],\n",
      "         [ 0.0428],\n",
      "         [-0.1226],\n",
      "         [-0.2569],\n",
      "         [-0.3167],\n",
      "         [-0.3633],\n",
      "         [-0.4106],\n",
      "         [-0.4183]]], grad_fn=<StackBackward>)\n",
      "tensor(0.3343, grad_fn=<MseLossBackward>)\n",
      "loss : 0.334281\n",
      "10\n",
      "tensor([[[-0.2795],\n",
      "         [-0.2585],\n",
      "         [-0.2716],\n",
      "         [-0.1743],\n",
      "         [ 0.0569],\n",
      "         [ 0.2402],\n",
      "         [ 0.3254],\n",
      "         [ 0.3763],\n",
      "         [ 0.4141],\n",
      "         [ 0.4019]]], grad_fn=<StackBackward>)\n",
      "tensor(0.2208, grad_fn=<MseLossBackward>)\n",
      "loss : 0.220759\n",
      "10\n",
      "tensor([[[ 0.2124],\n",
      "         [ 0.1714],\n",
      "         [ 0.1669],\n",
      "         [ 0.0072],\n",
      "         [-0.3035],\n",
      "         [-0.5488],\n",
      "         [-0.6858],\n",
      "         [-0.7833],\n",
      "         [-0.8497],\n",
      "         [-0.8508]]], grad_fn=<StackBackward>)\n",
      "tensor(0.2246, grad_fn=<MseLossBackward>)\n",
      "loss : 0.224590\n",
      "10\n",
      "tensor([[[-0.6965],\n",
      "         [-0.6754],\n",
      "         [-0.6463],\n",
      "         [-0.4091],\n",
      "         [ 0.0273],\n",
      "         [ 0.4407],\n",
      "         [ 0.7179],\n",
      "         [ 0.8746],\n",
      "         [ 0.9579],\n",
      "         [ 0.9864]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0356, grad_fn=<MseLossBackward>)\n",
      "loss : 0.035607\n",
      "10\n",
      "tensor([[[ 0.8804],\n",
      "         [ 0.9707],\n",
      "         [ 1.0609],\n",
      "         [ 0.9599],\n",
      "         [ 0.6770],\n",
      "         [ 0.2819],\n",
      "         [-0.2046],\n",
      "         [-0.6942],\n",
      "         [-1.0442],\n",
      "         [-1.2299]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward>)\n",
      "loss : 0.093075\n",
      "10\n",
      "tensor([[[-1.2340],\n",
      "         [-1.2973],\n",
      "         [-1.3191],\n",
      "         [-1.1633],\n",
      "         [-0.7883],\n",
      "         [-0.2396],\n",
      "         [ 0.4025],\n",
      "         [ 0.9190],\n",
      "         [ 1.1848],\n",
      "         [ 1.2897]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1654, grad_fn=<MseLossBackward>)\n",
      "loss : 0.165385\n",
      "10\n",
      "tensor([[[ 1.1833e+00],\n",
      "         [ 1.0577e+00],\n",
      "         [ 9.0429e-01],\n",
      "         [ 5.7347e-01],\n",
      "         [-6.4302e-04],\n",
      "         [-5.8322e-01],\n",
      "         [-9.8289e-01],\n",
      "         [-1.1850e+00],\n",
      "         [-1.2847e+00],\n",
      "         [-1.3279e+00]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0905, grad_fn=<MseLossBackward>)\n",
      "loss : 0.090540\n",
      "10\n",
      "tensor([[[-1.0088],\n",
      "         [-0.7656],\n",
      "         [-0.5807],\n",
      "         [-0.1974],\n",
      "         [ 0.3830],\n",
      "         [ 0.7363],\n",
      "         [ 0.8547],\n",
      "         [ 0.9310],\n",
      "         [ 1.0068],\n",
      "         [ 1.0259]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0941, grad_fn=<MseLossBackward>)\n",
      "loss : 0.094085\n",
      "10\n",
      "tensor([[[ 0.6404],\n",
      "         [ 0.4973],\n",
      "         [ 0.5419],\n",
      "         [ 0.3369],\n",
      "         [-0.1346],\n",
      "         [-0.4314],\n",
      "         [-0.5993],\n",
      "         [-0.7726],\n",
      "         [-0.8813],\n",
      "         [-0.8932]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0588, grad_fn=<MseLossBackward>)\n",
      "loss : 0.058802\n",
      "10\n",
      "tensor([[[-0.5179],\n",
      "         [-0.4305],\n",
      "         [-0.4641],\n",
      "         [-0.1906],\n",
      "         [ 0.2221],\n",
      "         [ 0.3698],\n",
      "         [ 0.4616],\n",
      "         [ 0.6167],\n",
      "         [ 0.7187],\n",
      "         [ 0.7569]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1006, grad_fn=<MseLossBackward>)\n",
      "loss : 0.100552\n",
      "10\n",
      "tensor([[[ 0.4910],\n",
      "         [ 0.5446],\n",
      "         [ 0.6797],\n",
      "         [ 0.5118],\n",
      "         [ 0.1936],\n",
      "         [ 0.0079],\n",
      "         [-0.2215],\n",
      "         [-0.4688],\n",
      "         [-0.5828],\n",
      "         [-0.6467]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0874, grad_fn=<MseLossBackward>)\n",
      "loss : 0.087419\n",
      "10\n",
      "tensor([[[-0.4662],\n",
      "         [-0.5267],\n",
      "         [-0.5745],\n",
      "         [-0.3408],\n",
      "         [-0.0400],\n",
      "         [ 0.1318],\n",
      "         [ 0.3024],\n",
      "         [ 0.4585],\n",
      "         [ 0.5614],\n",
      "         [ 0.6667]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0925, grad_fn=<MseLossBackward>)\n",
      "loss : 0.092496\n",
      "10\n",
      "tensor([[[ 0.5642],\n",
      "         [ 0.6664],\n",
      "         [ 0.7605],\n",
      "         [ 0.6101],\n",
      "         [ 0.3568],\n",
      "         [ 0.1213],\n",
      "         [-0.1681],\n",
      "         [-0.4230],\n",
      "         [-0.5713],\n",
      "         [-0.6935]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0855, grad_fn=<MseLossBackward>)\n",
      "loss : 0.085477\n",
      "10\n",
      "tensor([[[-0.6257],\n",
      "         [-0.7256],\n",
      "         [-0.7426],\n",
      "         [-0.4936],\n",
      "         [-0.1762],\n",
      "         [ 0.1219],\n",
      "         [ 0.3727],\n",
      "         [ 0.5337],\n",
      "         [ 0.6527],\n",
      "         [ 0.7772]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0391, grad_fn=<MseLossBackward>)\n",
      "loss : 0.039143\n",
      "10\n",
      "tensor([[[ 0.7314],\n",
      "         [ 0.7974],\n",
      "         [ 0.7963],\n",
      "         [ 0.5638],\n",
      "         [ 0.2193],\n",
      "         [-0.1428],\n",
      "         [-0.4645],\n",
      "         [-0.6890],\n",
      "         [-0.8347],\n",
      "         [-0.9505]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0121, grad_fn=<MseLossBackward>)\n",
      "loss : 0.012109\n",
      "10\n",
      "tensor([[[-0.8810],\n",
      "         [-0.9389],\n",
      "         [-0.8880],\n",
      "         [-0.5719],\n",
      "         [-0.1430],\n",
      "         [ 0.2995],\n",
      "         [ 0.6081],\n",
      "         [ 0.7697],\n",
      "         [ 0.8781],\n",
      "         [ 0.9691]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "loss : 0.006743\n",
      "10\n",
      "tensor([[[ 0.8508],\n",
      "         [ 0.8204],\n",
      "         [ 0.7196],\n",
      "         [ 0.3768],\n",
      "         [-0.1043],\n",
      "         [-0.5397],\n",
      "         [-0.8215],\n",
      "         [-0.9871],\n",
      "         [-1.1072],\n",
      "         [-1.1951]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0483, grad_fn=<MseLossBackward>)\n",
      "loss : 0.048344\n",
      "10\n",
      "tensor([[[-1.0050],\n",
      "         [-0.9998],\n",
      "         [-0.9544],\n",
      "         [-0.6316],\n",
      "         [-0.1273],\n",
      "         [ 0.3998],\n",
      "         [ 0.7625],\n",
      "         [ 0.9413],\n",
      "         [ 1.0353],\n",
      "         [ 1.1015]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0229, grad_fn=<MseLossBackward>)\n",
      "loss : 0.022875\n",
      "10\n",
      "tensor([[[ 0.8794],\n",
      "         [ 0.8334],\n",
      "         [ 0.7869],\n",
      "         [ 0.4861],\n",
      "         [-0.0070],\n",
      "         [-0.4932],\n",
      "         [-0.8338],\n",
      "         [-1.0356],\n",
      "         [-1.1647],\n",
      "         [-1.2593]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0463, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.046319\n",
      "10\n",
      "tensor([[[-0.9993],\n",
      "         [-1.0030],\n",
      "         [-1.0418],\n",
      "         [-0.8181],\n",
      "         [-0.3831],\n",
      "         [ 0.1391],\n",
      "         [ 0.6013],\n",
      "         [ 0.8933],\n",
      "         [ 1.0326],\n",
      "         [ 1.1201]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0276, grad_fn=<MseLossBackward>)\n",
      "loss : 0.027580\n",
      "10\n",
      "tensor([[[ 0.9000],\n",
      "         [ 0.9135],\n",
      "         [ 0.9645],\n",
      "         [ 0.7902],\n",
      "         [ 0.4234],\n",
      "         [-0.0207],\n",
      "         [-0.4563],\n",
      "         [-0.7972],\n",
      "         [-0.9936],\n",
      "         [-1.1117]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0238, grad_fn=<MseLossBackward>)\n",
      "loss : 0.023829\n",
      "10\n",
      "tensor([[[-0.8866],\n",
      "         [-0.8680],\n",
      "         [-0.8963],\n",
      "         [-0.6961],\n",
      "         [-0.2890],\n",
      "         [ 0.1515],\n",
      "         [ 0.5296],\n",
      "         [ 0.7826],\n",
      "         [ 0.9164],\n",
      "         [ 1.0167]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "loss : 0.008920\n",
      "10\n",
      "tensor([[[ 0.8147],\n",
      "         [ 0.7990],\n",
      "         [ 0.8371],\n",
      "         [ 0.6749],\n",
      "         [ 0.3271],\n",
      "         [-0.0488],\n",
      "         [-0.3876],\n",
      "         [-0.6576],\n",
      "         [-0.8207],\n",
      "         [-0.9199]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0174, grad_fn=<MseLossBackward>)\n",
      "loss : 0.017389\n",
      "10\n",
      "tensor([[[-0.6892],\n",
      "         [-0.6372],\n",
      "         [-0.6359],\n",
      "         [-0.4121],\n",
      "         [-0.0231],\n",
      "         [ 0.3230],\n",
      "         [ 0.5543],\n",
      "         [ 0.7061],\n",
      "         [ 0.8153],\n",
      "         [ 0.8987]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0290, grad_fn=<MseLossBackward>)\n",
      "loss : 0.028999\n",
      "10\n",
      "tensor([[[ 0.6700],\n",
      "         [ 0.6262],\n",
      "         [ 0.6439],\n",
      "         [ 0.4497],\n",
      "         [ 0.0986],\n",
      "         [-0.2274],\n",
      "         [-0.4611],\n",
      "         [-0.6345],\n",
      "         [-0.7574],\n",
      "         [-0.8300]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0314, grad_fn=<MseLossBackward>)\n",
      "loss : 0.031409\n",
      "10\n",
      "tensor([[[-0.5745],\n",
      "         [-0.5388],\n",
      "         [-0.5562],\n",
      "         [-0.3260],\n",
      "         [ 0.0387],\n",
      "         [ 0.3637],\n",
      "         [ 0.5645],\n",
      "         [ 0.6882],\n",
      "         [ 0.7884],\n",
      "         [ 0.8524]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0552, grad_fn=<MseLossBackward>)\n",
      "loss : 0.055214\n",
      "10\n",
      "tensor([[[ 0.6090],\n",
      "         [ 0.5802],\n",
      "         [ 0.5988],\n",
      "         [ 0.3779],\n",
      "         [ 0.0269],\n",
      "         [-0.3033],\n",
      "         [-0.5276],\n",
      "         [-0.6830],\n",
      "         [-0.8063],\n",
      "         [-0.8797]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0403, grad_fn=<MseLossBackward>)\n",
      "loss : 0.040329\n",
      "10\n",
      "tensor([[[-0.6373],\n",
      "         [-0.6470],\n",
      "         [-0.6888],\n",
      "         [-0.4624],\n",
      "         [-0.0974],\n",
      "         [ 0.3051],\n",
      "         [ 0.5938],\n",
      "         [ 0.7568],\n",
      "         [ 0.8676],\n",
      "         [ 0.9308]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0267, grad_fn=<MseLossBackward>)\n",
      "loss : 0.026653\n",
      "10\n",
      "tensor([[[ 0.7135],\n",
      "         [ 0.7244],\n",
      "         [ 0.7486],\n",
      "         [ 0.5237],\n",
      "         [ 0.1659],\n",
      "         [-0.2446],\n",
      "         [-0.5654],\n",
      "         [-0.7778],\n",
      "         [-0.9339],\n",
      "         [-1.0340]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "loss : 0.014004\n",
      "10\n",
      "tensor([[[-0.8425],\n",
      "         [-0.8965],\n",
      "         [-0.9537],\n",
      "         [-0.7694],\n",
      "         [-0.4335],\n",
      "         [ 0.0600],\n",
      "         [ 0.5158],\n",
      "         [ 0.8289],\n",
      "         [ 1.0168],\n",
      "         [ 1.1047]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0236, grad_fn=<MseLossBackward>)\n",
      "loss : 0.023605\n",
      "10\n",
      "tensor([[[ 0.9272],\n",
      "         [ 0.9345],\n",
      "         [ 0.9408],\n",
      "         [ 0.7409],\n",
      "         [ 0.3847],\n",
      "         [-0.0894],\n",
      "         [-0.5228],\n",
      "         [-0.8331],\n",
      "         [-1.0397],\n",
      "         [-1.1657]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0188, grad_fn=<MseLossBackward>)\n",
      "loss : 0.018794\n",
      "10\n",
      "tensor([[[-0.9821],\n",
      "         [-0.9494],\n",
      "         [-0.9242],\n",
      "         [-0.7150],\n",
      "         [-0.3320],\n",
      "         [ 0.1813],\n",
      "         [ 0.6331],\n",
      "         [ 0.9285],\n",
      "         [ 1.0935],\n",
      "         [ 1.1694]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0193, grad_fn=<MseLossBackward>)\n",
      "loss : 0.019327\n",
      "10\n",
      "tensor([[[ 0.9187],\n",
      "         [ 0.7836],\n",
      "         [ 0.6913],\n",
      "         [ 0.4528],\n",
      "         [ 0.0681],\n",
      "         [-0.3432],\n",
      "         [-0.6608],\n",
      "         [-0.8738],\n",
      "         [-1.0250],\n",
      "         [-1.1173]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "loss : 0.013716\n",
      "10\n",
      "tensor([[[-0.8307],\n",
      "         [-0.6922],\n",
      "         [-0.6278],\n",
      "         [-0.4233],\n",
      "         [-0.0778],\n",
      "         [ 0.3355],\n",
      "         [ 0.6646],\n",
      "         [ 0.8732],\n",
      "         [ 0.9954],\n",
      "         [ 1.0429]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0194, grad_fn=<MseLossBackward>)\n",
      "loss : 0.019380\n",
      "10\n",
      "tensor([[[ 0.7303],\n",
      "         [ 0.6179],\n",
      "         [ 0.6077],\n",
      "         [ 0.4454],\n",
      "         [ 0.1571],\n",
      "         [-0.1968],\n",
      "         [-0.4956],\n",
      "         [-0.7172],\n",
      "         [-0.8835],\n",
      "         [-0.9710]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0212, grad_fn=<MseLossBackward>)\n",
      "loss : 0.021159\n",
      "10\n",
      "tensor([[[-0.6857],\n",
      "         [-0.6058],\n",
      "         [-0.6267],\n",
      "         [-0.4923],\n",
      "         [-0.2314],\n",
      "         [ 0.1455],\n",
      "         [ 0.4921],\n",
      "         [ 0.7434],\n",
      "         [ 0.9146],\n",
      "         [ 0.9870]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0235, grad_fn=<MseLossBackward>)\n",
      "loss : 0.023518\n",
      "10\n",
      "tensor([[[ 0.7342],\n",
      "         [ 0.7071],\n",
      "         [ 0.7541],\n",
      "         [ 0.6381],\n",
      "         [ 0.4109],\n",
      "         [ 0.0623],\n",
      "         [-0.2804],\n",
      "         [-0.5725],\n",
      "         [-0.8079],\n",
      "         [-0.9386]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0363, grad_fn=<MseLossBackward>)\n",
      "loss : 0.036272\n",
      "10\n",
      "tensor([[[-0.7307],\n",
      "         [-0.7044],\n",
      "         [-0.7272],\n",
      "         [-0.5942],\n",
      "         [-0.3480],\n",
      "         [ 0.0522],\n",
      "         [ 0.4400],\n",
      "         [ 0.7412],\n",
      "         [ 0.9506],\n",
      "         [ 1.0267]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0188, grad_fn=<MseLossBackward>)\n",
      "loss : 0.018846\n",
      "10\n",
      "tensor([[[ 0.7812],\n",
      "         [ 0.6564],\n",
      "         [ 0.5623],\n",
      "         [ 0.3597],\n",
      "         [ 0.0808],\n",
      "         [-0.2672],\n",
      "         [-0.5757],\n",
      "         [-0.8200],\n",
      "         [-1.0089],\n",
      "         [-1.1040]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0231, grad_fn=<MseLossBackward>)\n",
      "loss : 0.023090\n",
      "10\n",
      "tensor([[[-0.8592],\n",
      "         [-0.7538],\n",
      "         [-0.6740],\n",
      "         [-0.4720],\n",
      "         [-0.1895],\n",
      "         [ 0.2398],\n",
      "         [ 0.6309],\n",
      "         [ 0.9246],\n",
      "         [ 1.1138],\n",
      "         [ 1.1497]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0163, grad_fn=<MseLossBackward>)\n",
      "loss : 0.016329\n",
      "10\n",
      "tensor([[[ 0.8552],\n",
      "         [ 0.7007],\n",
      "         [ 0.5914],\n",
      "         [ 0.3878],\n",
      "         [ 0.1258],\n",
      "         [-0.2342],\n",
      "         [-0.5771],\n",
      "         [-0.8669],\n",
      "         [-1.0967],\n",
      "         [-1.2031]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0209, grad_fn=<MseLossBackward>)\n",
      "loss : 0.020911\n",
      "10\n",
      "tensor([[[-0.9409],\n",
      "         [-0.8185],\n",
      "         [-0.7417],\n",
      "         [-0.5712],\n",
      "         [-0.3404],\n",
      "         [ 0.0606],\n",
      "         [ 0.4847],\n",
      "         [ 0.8636],\n",
      "         [ 1.1576],\n",
      "         [ 1.2749]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "loss : 0.019722\n",
      "10\n",
      "tensor([[[ 1.0432],\n",
      "         [ 0.9588],\n",
      "         [ 0.9144],\n",
      "         [ 0.7872],\n",
      "         [ 0.6013],\n",
      "         [ 0.2684],\n",
      "         [-0.1152],\n",
      "         [-0.5106],\n",
      "         [-0.8765],\n",
      "         [-1.1009]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0713, grad_fn=<MseLossBackward>)\n",
      "loss : 0.071255\n",
      "10\n",
      "tensor([[[-0.9051],\n",
      "         [-0.7028],\n",
      "         [-0.4781],\n",
      "         [-0.2215],\n",
      "         [ 0.0095],\n",
      "         [ 0.2788],\n",
      "         [ 0.5221],\n",
      "         [ 0.7380],\n",
      "         [ 0.8953],\n",
      "         [ 0.9212]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0280, grad_fn=<MseLossBackward>)\n",
      "loss : 0.027964\n",
      "10\n",
      "tensor([[[ 0.4846],\n",
      "         [ 0.0388],\n",
      "         [-0.2823],\n",
      "         [-0.4382],\n",
      "         [-0.4212],\n",
      "         [-0.4165],\n",
      "         [-0.5192],\n",
      "         [-0.7397],\n",
      "         [-1.0149],\n",
      "         [-1.1826]]], grad_fn=<StackBackward>)\n",
      "tensor(0.3509, grad_fn=<MseLossBackward>)\n",
      "loss : 0.350933\n",
      "10\n",
      "tensor([[[-0.8230],\n",
      "         [-0.5524],\n",
      "         [-0.3694],\n",
      "         [-0.2079],\n",
      "         [-0.1606],\n",
      "         [ 0.0035],\n",
      "         [ 0.2044],\n",
      "         [ 0.4579],\n",
      "         [ 0.7232],\n",
      "         [ 0.7926]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0725, grad_fn=<MseLossBackward>)\n",
      "loss : 0.072533\n",
      "10\n",
      "tensor([[[ 0.4566],\n",
      "         [ 0.3005],\n",
      "         [ 0.2469],\n",
      "         [ 0.1609],\n",
      "         [ 0.1129],\n",
      "         [-0.0879],\n",
      "         [-0.3388],\n",
      "         [-0.6476],\n",
      "         [-0.9908],\n",
      "         [-1.1932]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1179, grad_fn=<MseLossBackward>)\n",
      "loss : 0.117932\n",
      "10\n",
      "tensor([[[-0.9907],\n",
      "         [-0.9731],\n",
      "         [-0.9776],\n",
      "         [-0.8481],\n",
      "         [-0.7043],\n",
      "         [-0.3342],\n",
      "         [ 0.1063],\n",
      "         [ 0.5903],\n",
      "         [ 1.1161],\n",
      "         [ 1.4032]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1086, grad_fn=<MseLossBackward>)\n",
      "loss : 0.108610\n",
      "10\n",
      "tensor([[[ 1.3377],\n",
      "         [ 1.3722],\n",
      "         [ 1.3109],\n",
      "         [ 1.1417],\n",
      "         [ 0.9344],\n",
      "         [ 0.5969],\n",
      "         [ 0.2312],\n",
      "         [-0.1929],\n",
      "         [-0.6654],\n",
      "         [-1.0384]]], grad_fn=<StackBackward>)\n",
      "tensor(0.3122, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.312221\n",
      "10\n",
      "tensor([[[-0.9158],\n",
      "         [-0.6394],\n",
      "         [-0.1845],\n",
      "         [ 0.3685],\n",
      "         [ 0.7583],\n",
      "         [ 0.9571],\n",
      "         [ 0.9944],\n",
      "         [ 0.9532],\n",
      "         [ 0.9094],\n",
      "         [ 0.8247]]], grad_fn=<StackBackward>)\n",
      "tensor(0.2983, grad_fn=<MseLossBackward>)\n",
      "loss : 0.298313\n",
      "10\n",
      "tensor([[[ 0.2272],\n",
      "         [-0.2179],\n",
      "         [-0.3154],\n",
      "         [-0.2349],\n",
      "         [-0.0315],\n",
      "         [-0.0227],\n",
      "         [-0.2803],\n",
      "         [-0.6393],\n",
      "         [-0.9958],\n",
      "         [-1.0719]]], grad_fn=<StackBackward>)\n",
      "tensor(0.3784, grad_fn=<MseLossBackward>)\n",
      "loss : 0.378445\n",
      "10\n",
      "tensor([[[-0.3546],\n",
      "         [ 0.2253],\n",
      "         [ 0.4373],\n",
      "         [ 0.3093],\n",
      "         [-0.0532],\n",
      "         [-0.1415],\n",
      "         [ 0.1061],\n",
      "         [ 0.5922],\n",
      "         [ 1.0202],\n",
      "         [ 0.9993]]], grad_fn=<StackBackward>)\n",
      "tensor(0.4182, grad_fn=<MseLossBackward>)\n",
      "loss : 0.418239\n",
      "10\n",
      "tensor([[[ 0.1712],\n",
      "         [-0.3445],\n",
      "         [-0.2946],\n",
      "         [-0.0849],\n",
      "         [ 0.1291],\n",
      "         [ 0.0103],\n",
      "         [-0.3062],\n",
      "         [-0.5711],\n",
      "         [-0.8026],\n",
      "         [-0.8016]]], grad_fn=<StackBackward>)\n",
      "tensor(0.3973, grad_fn=<MseLossBackward>)\n",
      "loss : 0.397286\n",
      "10\n",
      "tensor([[[-0.1974],\n",
      "         [-0.0980],\n",
      "         [-0.2659],\n",
      "         [-0.3180],\n",
      "         [-0.2961],\n",
      "         [ 0.0090],\n",
      "         [ 0.2900],\n",
      "         [ 0.4790],\n",
      "         [ 0.6172],\n",
      "         [ 0.5428]]], grad_fn=<StackBackward>)\n",
      "tensor(0.2118, grad_fn=<MseLossBackward>)\n",
      "loss : 0.211762\n",
      "10\n",
      "tensor([[[ 0.0846],\n",
      "         [ 0.0457],\n",
      "         [ 0.2225],\n",
      "         [ 0.2193],\n",
      "         [ 0.1858],\n",
      "         [ 0.0735],\n",
      "         [-0.1110],\n",
      "         [-0.2821],\n",
      "         [-0.5088],\n",
      "         [-0.6917]]], grad_fn=<StackBackward>)\n",
      "tensor(0.2739, grad_fn=<MseLossBackward>)\n",
      "loss : 0.273879\n",
      "10\n",
      "tensor([[[-0.3271],\n",
      "         [-0.4772],\n",
      "         [-0.6835],\n",
      "         [-0.5221],\n",
      "         [-0.4328],\n",
      "         [-0.1509],\n",
      "         [ 0.1323],\n",
      "         [ 0.2686],\n",
      "         [ 0.5405],\n",
      "         [ 0.6312]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1525, grad_fn=<MseLossBackward>)\n",
      "loss : 0.152452\n",
      "10\n",
      "tensor([[[ 0.2745],\n",
      "         [ 0.3672],\n",
      "         [ 0.4318],\n",
      "         [ 0.3303],\n",
      "         [ 0.3398],\n",
      "         [ 0.2416],\n",
      "         [ 0.0627],\n",
      "         [-0.1094],\n",
      "         [-0.3782],\n",
      "         [-0.6482]]], grad_fn=<StackBackward>)\n",
      "tensor(0.2382, grad_fn=<MseLossBackward>)\n",
      "loss : 0.238163\n",
      "10\n",
      "tensor([[[-0.4425],\n",
      "         [-0.6956],\n",
      "         [-0.9082],\n",
      "         [-0.6808],\n",
      "         [-0.5470],\n",
      "         [-0.2502],\n",
      "         [ 0.1468],\n",
      "         [ 0.2948],\n",
      "         [ 0.6113],\n",
      "         [ 0.8265]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1227, grad_fn=<MseLossBackward>)\n",
      "loss : 0.122711\n",
      "10\n",
      "tensor([[[ 0.4431],\n",
      "         [ 0.5534],\n",
      "         [ 0.5345],\n",
      "         [ 0.2763],\n",
      "         [ 0.2956],\n",
      "         [ 0.1606],\n",
      "         [-0.0790],\n",
      "         [-0.2123],\n",
      "         [-0.5152],\n",
      "         [-0.8147]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1388, grad_fn=<MseLossBackward>)\n",
      "loss : 0.138800\n",
      "10\n",
      "tensor([[[-0.6514],\n",
      "         [-0.9037],\n",
      "         [-1.0355],\n",
      "         [-0.6843],\n",
      "         [-0.4116],\n",
      "         [-0.0217],\n",
      "         [ 0.4645],\n",
      "         [ 0.6121],\n",
      "         [ 0.8720],\n",
      "         [ 1.0667]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0358, grad_fn=<MseLossBackward>)\n",
      "loss : 0.035818\n",
      "10\n",
      "tensor([[[ 0.6083],\n",
      "         [ 0.5880],\n",
      "         [ 0.4439],\n",
      "         [ 0.0385],\n",
      "         [-0.0382],\n",
      "         [-0.2001],\n",
      "         [-0.4828],\n",
      "         [-0.6018],\n",
      "         [-0.8484],\n",
      "         [-1.1024]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0686, grad_fn=<MseLossBackward>)\n",
      "loss : 0.068553\n",
      "10\n",
      "tensor([[[-0.8432],\n",
      "         [-0.9533],\n",
      "         [-0.9441],\n",
      "         [-0.4329],\n",
      "         [ 0.0131],\n",
      "         [ 0.4485],\n",
      "         [ 0.8894],\n",
      "         [ 0.9973],\n",
      "         [ 1.1417],\n",
      "         [ 1.2458]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0478, grad_fn=<MseLossBackward>)\n",
      "loss : 0.047763\n",
      "10\n",
      "tensor([[[ 0.7200],\n",
      "         [ 0.5958],\n",
      "         [ 0.3924],\n",
      "         [-0.0609],\n",
      "         [-0.2422],\n",
      "         [-0.4226],\n",
      "         [-0.6902],\n",
      "         [-0.8321],\n",
      "         [-1.0344],\n",
      "         [-1.2373]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0992, grad_fn=<MseLossBackward>)\n",
      "loss : 0.099156\n",
      "10\n",
      "tensor([[[-0.8689],\n",
      "         [-0.9092],\n",
      "         [-0.8629],\n",
      "         [-0.3502],\n",
      "         [ 0.1314],\n",
      "         [ 0.5811],\n",
      "         [ 0.9840],\n",
      "         [ 1.1346],\n",
      "         [ 1.2895],\n",
      "         [ 1.4074]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0967, grad_fn=<MseLossBackward>)\n",
      "loss : 0.096743\n",
      "10\n",
      "tensor([[[ 0.9350],\n",
      "         [ 0.9431],\n",
      "         [ 0.8696],\n",
      "         [ 0.4844],\n",
      "         [ 0.2158],\n",
      "         [-0.0721],\n",
      "         [-0.4067],\n",
      "         [-0.6527],\n",
      "         [-0.9145],\n",
      "         [-1.1639]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "loss : 0.007632\n",
      "10\n",
      "tensor([[[-0.8274],\n",
      "         [-0.9589],\n",
      "         [-1.0138],\n",
      "         [-0.6507],\n",
      "         [-0.2345],\n",
      "         [ 0.2644],\n",
      "         [ 0.7310],\n",
      "         [ 1.0086],\n",
      "         [ 1.2350],\n",
      "         [ 1.4272]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0508, grad_fn=<MseLossBackward>)\n",
      "loss : 0.050807\n",
      "10\n",
      "tensor([[[ 1.0593],\n",
      "         [ 1.1686],\n",
      "         [ 1.1938],\n",
      "         [ 0.9166],\n",
      "         [ 0.6406],\n",
      "         [ 0.3317],\n",
      "         [-0.0154],\n",
      "         [-0.3527],\n",
      "         [-0.6790],\n",
      "         [-0.9842]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1360, grad_fn=<MseLossBackward>)\n",
      "loss : 0.135982\n",
      "10\n",
      "tensor([[[-0.7460],\n",
      "         [-0.8985],\n",
      "         [-0.9509],\n",
      "         [-0.6371],\n",
      "         [-0.2480],\n",
      "         [ 0.1849],\n",
      "         [ 0.5751],\n",
      "         [ 0.8556],\n",
      "         [ 1.0607],\n",
      "         [ 1.2405]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0211, grad_fn=<MseLossBackward>)\n",
      "loss : 0.021093\n",
      "10\n",
      "tensor([[[ 0.8479],\n",
      "         [ 0.8425],\n",
      "         [ 0.7568],\n",
      "         [ 0.4439],\n",
      "         [ 0.1923],\n",
      "         [-0.0202],\n",
      "         [-0.2601],\n",
      "         [-0.5104],\n",
      "         [-0.7223],\n",
      "         [-0.9479]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0233, grad_fn=<MseLossBackward>)\n",
      "loss : 0.023260\n",
      "10\n",
      "tensor([[[-0.6671],\n",
      "         [-0.7522],\n",
      "         [-0.7377],\n",
      "         [-0.4097],\n",
      "         [-0.0349],\n",
      "         [ 0.2981],\n",
      "         [ 0.5558],\n",
      "         [ 0.7608],\n",
      "         [ 0.8867],\n",
      "         [ 0.9917]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0196, grad_fn=<MseLossBackward>)\n",
      "loss : 0.019570\n",
      "10\n",
      "tensor([[[ 0.5133],\n",
      "         [ 0.4382],\n",
      "         [ 0.3199],\n",
      "         [ 0.0570],\n",
      "         [-0.1127],\n",
      "         [-0.2504],\n",
      "         [-0.4081],\n",
      "         [-0.6231],\n",
      "         [-0.7808],\n",
      "         [-0.9599]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1027, grad_fn=<MseLossBackward>)\n",
      "loss : 0.102723\n",
      "10\n",
      "tensor([[[-0.6175],\n",
      "         [-0.6836],\n",
      "         [-0.6561],\n",
      "         [-0.3568],\n",
      "         [-0.0016],\n",
      "         [ 0.3012],\n",
      "         [ 0.5129],\n",
      "         [ 0.7015],\n",
      "         [ 0.7909],\n",
      "         [ 0.8643]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0335, grad_fn=<MseLossBackward>)\n",
      "loss : 0.033519\n",
      "10\n",
      "tensor([[[ 0.3680],\n",
      "         [ 0.3122],\n",
      "         [ 0.2191],\n",
      "         [ 0.0053],\n",
      "         [-0.1554],\n",
      "         [-0.2925],\n",
      "         [-0.4376],\n",
      "         [-0.6616],\n",
      "         [-0.8160],\n",
      "         [-0.9996]]], grad_fn=<StackBackward>)\n",
      "tensor(0.1489, grad_fn=<MseLossBackward>)\n",
      "loss : 0.148942\n",
      "10\n",
      "tensor([[[-0.6428],\n",
      "         [-0.7525],\n",
      "         [-0.7482],\n",
      "         [-0.4975],\n",
      "         [-0.1220],\n",
      "         [ 0.2150],\n",
      "         [ 0.4673],\n",
      "         [ 0.6919],\n",
      "         [ 0.8032],\n",
      "         [ 0.9053]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0202, grad_fn=<MseLossBackward>)\n",
      "loss : 0.020153\n",
      "10\n",
      "tensor([[[ 0.4622],\n",
      "         [ 0.4676],\n",
      "         [ 0.3912],\n",
      "         [ 0.1829],\n",
      "         [-0.0334],\n",
      "         [-0.2059],\n",
      "         [-0.3820],\n",
      "         [-0.6310],\n",
      "         [-0.8213],\n",
      "         [-1.0422]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0845, grad_fn=<MseLossBackward>)\n",
      "loss : 0.084502\n",
      "10\n",
      "tensor([[[-0.7352],\n",
      "         [-0.9019],\n",
      "         [-0.9337],\n",
      "         [-0.7347],\n",
      "         [-0.3330],\n",
      "         [ 0.0599],\n",
      "         [ 0.4147],\n",
      "         [ 0.7098],\n",
      "         [ 0.8925],\n",
      "         [ 1.0473]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0208, grad_fn=<MseLossBackward>)\n",
      "loss : 0.020798\n",
      "10\n",
      "tensor([[[ 0.7054],\n",
      "         [ 0.7661],\n",
      "         [ 0.7193],\n",
      "         [ 0.5084],\n",
      "         [ 0.2205],\n",
      "         [-0.0245],\n",
      "         [-0.2789],\n",
      "         [-0.5746],\n",
      "         [-0.8267],\n",
      "         [-1.0825]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0249, grad_fn=<MseLossBackward>)\n",
      "loss : 0.024871\n",
      "10\n",
      "tensor([[[-0.8555],\n",
      "         [-1.0092],\n",
      "         [-1.0369],\n",
      "         [-0.8229],\n",
      "         [-0.3883],\n",
      "         [ 0.0484],\n",
      "         [ 0.4637],\n",
      "         [ 0.7872],\n",
      "         [ 1.0194],\n",
      "         [ 1.1919]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0310, grad_fn=<MseLossBackward>)\n",
      "loss : 0.031015\n",
      "10\n",
      "tensor([[[ 0.9096],\n",
      "         [ 0.9396],\n",
      "         [ 0.8852],\n",
      "         [ 0.6451],\n",
      "         [ 0.3160],\n",
      "         [ 0.0160],\n",
      "         [-0.2887],\n",
      "         [-0.6102],\n",
      "         [-0.8882],\n",
      "         [-1.1367]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0190, grad_fn=<MseLossBackward>)\n",
      "loss : 0.018994\n",
      "10\n",
      "tensor([[[-0.9093],\n",
      "         [-0.9361],\n",
      "         [-0.8543],\n",
      "         [-0.5211],\n",
      "         [-0.0389],\n",
      "         [ 0.3540],\n",
      "         [ 0.6705],\n",
      "         [ 0.9092],\n",
      "         [ 1.1089],\n",
      "         [ 1.2447]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0205, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.020529\n",
      "10\n",
      "tensor([[[ 0.9031],\n",
      "         [ 0.8313],\n",
      "         [ 0.7224],\n",
      "         [ 0.4442],\n",
      "         [ 0.1193],\n",
      "         [-0.1654],\n",
      "         [-0.4305],\n",
      "         [-0.7148],\n",
      "         [-0.9526],\n",
      "         [-1.1518]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "loss : 0.005984\n",
      "10\n",
      "tensor([[[-0.8129],\n",
      "         [-0.7149],\n",
      "         [-0.5518],\n",
      "         [-0.2007],\n",
      "         [ 0.2145],\n",
      "         [ 0.4927],\n",
      "         [ 0.7249],\n",
      "         [ 0.9217],\n",
      "         [ 1.1094],\n",
      "         [ 1.2186]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0625, grad_fn=<MseLossBackward>)\n",
      "loss : 0.062485\n",
      "10\n",
      "tensor([[[ 0.8199],\n",
      "         [ 0.7143],\n",
      "         [ 0.6318],\n",
      "         [ 0.3878],\n",
      "         [ 0.1084],\n",
      "         [-0.1714],\n",
      "         [-0.4225],\n",
      "         [-0.7205],\n",
      "         [-0.9448],\n",
      "         [-1.1455]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "loss : 0.014738\n",
      "10\n",
      "tensor([[[-0.7491],\n",
      "         [-0.6892],\n",
      "         [-0.5783],\n",
      "         [-0.3266],\n",
      "         [ 0.0723],\n",
      "         [ 0.3370],\n",
      "         [ 0.6512],\n",
      "         [ 0.8427],\n",
      "         [ 1.0910],\n",
      "         [ 1.1760]]], grad_fn=<StackBackward>)\n",
      "tensor(0.0361, grad_fn=<MseLossBackward>)\n",
      "loss : 0.036078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-59b137be22a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss : %f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mioff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2210\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m                     \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2212\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2213\u001b[0m                 \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m                 \u001b[0mbbox_inches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/local/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[0;32m--> 532\u001b[0;31m                                self.figure.dpi, metadata=metadata)\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_dpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()\n",
    "h_state = torch.zeros(1, 1, h_size) # for initial hidden state\n",
    "plt.figure(2, figsize=(12, 5))\n",
    "plt.suptitle('Using sin input to predict cos output',fontsize='18')\n",
    "plt.ion() # continuously plot\n",
    "for step in range(100):\n",
    "    start, end = step * np.pi, (step+1)*np.pi # time range\n",
    "    # use sin predicts cos\n",
    "    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)\n",
    "    x_np = np.sin(steps) # float32 for converting torch FloatTensor\n",
    "    y_np = np.cos(steps)\n",
    "    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis]) # shape (batch, time_step, input_size)\n",
    "    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])\n",
    "    prediction, h_state = rnn(x, h_state) # rnn output\n",
    "    # !! next step is important !!\n",
    "    h_state = h_state.data # repack the hidden state, break the connection from last iteration\n",
    "    loss = loss_func(prediction, y) # cross entropy loss\n",
    "    print(prediction)\n",
    "    print(loss)\n",
    "    optimizer.zero_grad() # clear gradients for this training step\n",
    "    loss.backward() # backpropagation, compute gradients\n",
    "    optimizer.step() # apply gradients\n",
    "    # plotting\n",
    "    # plt.text(0, 0, 'Loss=%.4f' % loss, fontdict={'size': 16, 'color': 'red'})\n",
    "    plt.plot(steps, y_np.flatten(), 'r-')\n",
    "    plt.plot(steps, prediction.data.numpy().flatten(), 'b-')\n",
    "    plt.draw()\n",
    "    print('loss : %f'%(loss))\n",
    "plt.pause(0.1)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)\n",
    "x_np = np.sin(steps) # float32 for converting torch FloatTensor\n",
    "y_np = np.cos(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
